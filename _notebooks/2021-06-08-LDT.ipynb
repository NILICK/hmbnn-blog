{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399a8fef-adbb-4fcf-9cac-1a2bcbad38a1",
   "metadata": {},
   "source": [
    "# LDT Installing in ubuntu\n",
    "> This post is about install Land Data Toolkit(LDT) in ubuntu\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [LDT, merra-2, downscale, LIS, modeling]\n",
    "- image: images/lis.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bcb55a-95e3-4a4c-89de-e1ea1f24a145",
   "metadata": {},
   "source": [
    "In this post, I want to describe how to install LDT in ubuntu step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97b14b-3f64-4fbe-9fc0-2c300b362653",
   "metadata": {},
   "source": [
    "## Necessary Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb044c-0c4f-4969-854c-c64510971c01",
   "metadata": {},
   "source": [
    "1- First conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe920d-cbc0-45e4-9921-270ffd97f521",
   "metadata": {},
   "source": [
    "2- check installed packages:\n",
    "\n",
    "> apt list --installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deddf2dd-a770-4869-a74d-d87d5a03c10f",
   "metadata": {},
   "source": [
    "3- Install CMAKE\n",
    "\n",
    "> sudo apt install cmake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d478b9-8780-44e8-b283-31a8e42301a4",
   "metadata": {},
   "source": [
    "4- Installing GCC & Fortran on Ubuntu\n",
    "\n",
    "> sudo apt update\n",
    "\n",
    "> sudo apt install build-essential\n",
    "\n",
    "> sudo apt-get install manpages-dev\n",
    "\n",
    "> sudo apt install gcc gfortran\n",
    "\n",
    "> gcc --version\n",
    "\n",
    "> gfortran --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a70ae-d715-4531-be05-e143e10e3452",
   "metadata": {},
   "source": [
    "5- Check Perl version:\n",
    "\n",
    "> perl -v\n",
    "\n",
    "for install perl:\n",
    "\n",
    "> sudo apt-get install perl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a835c1-c199-4fec-897a-2f23ddd1984d",
   "metadata": {},
   "source": [
    "6- Install lapack:\n",
    "    \n",
    "> sudo apt-get install liblapack-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddde443-3d55-422b-a08a-74f61d94a3da",
   "metadata": {},
   "source": [
    "7- How to check if the MPI already installed on my machine?\n",
    "\n",
    "> ompi_info\n",
    "\n",
    "install:\n",
    "\n",
    "> sudo apt install openmpi-bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3298f80-674b-4706-a967-df5402b5a58c",
   "metadata": {},
   "source": [
    "8- Install NetCDF and NetCDF-Fortran (src:https://cloud-gc.readthedocs.io/en/stable/chapter04_developer-guide/install-basic.html)\n",
    "\n",
    "> sudo apt-get install libnetcdf-dev libnetcdff-dev\n",
    "\n",
    "Check NetCDF-C configuration:\n",
    "\n",
    "> nc-config --all\n",
    "\n",
    "Check NetCDF-Fortran configuration:\n",
    "\n",
    "> nf-config --all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a11e1-8f9e-4d16-930f-a662dbbad66f",
   "metadata": {},
   "source": [
    "9- Install  OpenJPEG:\n",
    "\n",
    "> sudo apt install libopenjp2-7\n",
    "\n",
    "> sudo apt-get install libopenjp2-7-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802ec8e-d1f9-448a-8b72-c457b85e0867",
   "metadata": {},
   "source": [
    "10- Install ecCodes:\n",
    "\n",
    "download the latest ecCodes from:\n",
    "\n",
    "https://confluence.ecmwf.int/display/ECC/Releases\n",
    "\n",
    "(src: https://confluence.ecmwf.int/display/ECC/ecCodes+installation)\n",
    "\n",
    "> tar -xzf  eccodes-x.y.z-Source.tar.gz\n",
    "\n",
    "> mkdir build ; cd build\n",
    "\n",
    "> cmake -DCMAKE_INSTALL+PREFIX=/home/nilik/MYPROGRAMS/LDT/ecCodes \n",
    "\n",
    "> ../eccodes-2.22.0-Source\n",
    "\n",
    "> ...\n",
    " \n",
    "> make\n",
    "\n",
    "> ctest\n",
    "\n",
    "> sudo make install\n",
    "\n",
    "then:\n",
    "\n",
    "> sudo apt install python3-pip\n",
    "\n",
    "> pip install eccodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f78934f-7c99-4388-9ead-2117a0d86e7f",
   "metadata": {},
   "source": [
    "11- Check for installed HDF4, HDF5, GDAL and GeoTIFF:\n",
    "\n",
    "> dpkg -l | grep hdf4\n",
    "\n",
    "> dpkg -l | grep hdf5\n",
    "\n",
    "> dpkg -l | grep gdal\n",
    "\n",
    "> dpkg -l | grep geotiff\n",
    "\n",
    "If each of them is not installed you can use the method from:\n",
    "\n",
    "https://github.com/NASA-LIS/LISF/blob/master/docs/LDT_users_guide/build.adoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff04838-ed4b-4f07-a0fd-ceae1395bf8f",
   "metadata": {},
   "source": [
    "12- For use Grib API it is necessary to instal jasper:\n",
    "    \n",
    "> sudo apt install jasper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02703785-db5f-4d8c-bb45-f54b40a5603e",
   "metadata": {},
   "source": [
    "13- sudo apt-get install libpng-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6811054-33a2-4ee7-bf93-b77112288029",
   "metadata": {},
   "source": [
    "14- sudo apt-get install zlib1g-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae8a45c-e7dc-472c-8ea3-79d33c9904d5",
   "metadata": {},
   "source": [
    "15- sudo apt-get install libjpeg8-dev\n",
    "\n",
    "16- sudo apt-get install flex bison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66f05c-024d-4cc4-9547-ab3a60bc7133",
   "metadata": {},
   "source": [
    "17- Install SZIP:\n",
    "\n",
    "> Download src from https://support.hdfgroup.org/ftp/lib-external/szip/2.1.1/src/szip-2.1.1.tar.gz\n",
    "\n",
    "Unzip and go in it within terminal:\n",
    "\n",
    "> ./configure –-prefix=/where_to_install (such as: /home/nilik/MYPROGRAMS/szip)\n",
    "\n",
    "> make\n",
    "\n",
    "> make check\n",
    "\n",
    "> make install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2688bf-5257-4eab-be5a-875926d6a5b1",
   "metadata": {},
   "source": [
    "18- Install HDF4\n",
    "\n",
    "Download HDF4.2.15 from https://portal.hdfgroup.org/display/support/HDF+4.2.15#files\n",
    "\n",
    "> Extract the source from the hdf-X.Y.Z.tar file and change directory to hdf-4.2.15\n",
    "\n",
    "[ *For prevent error:  configure: error: couldn't find rpc headers:*\n",
    "\n",
    "Replace HDF4 version from 4.2.13 to 4.2.15 to fix the error. Then you will get the next configure error:\n",
    "\n",
    "configure: error: couldn't find rpc headers\n",
    "\n",
    "To fix it edit \"configure\" file from HDF4 v4.3.15 sources and replace line 23672:\n",
    "\n",
    "```\n",
    "CPPFLAGS=\"$SYSCPPFLAGS -I/usr/include/tirpc\"\n",
    "\n",
    "```        \n",
    "to\n",
    "\n",
    "```\n",
    "CPPFLAGS=\"$SYSCPPFLAGS -I/usr/include/tirpc\"\n",
    "        \n",
    "unset ac_cv_header_rpc_rpc_h\n",
    "```       \n",
    "SRC: https://gitlab.orfeo-toolbox.org/maja/maja/-/issues/207 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaad0e6-c8a8-4ccf-a558-f96663142e67",
   "metadata": {},
   "source": [
    "- For Install HDF4:\n",
    "\n",
    "It is very very important first:\n",
    "> sudo -i\n",
    "\n",
    "Then:\n",
    "\n",
    "> cd /home/nilik/Temp/hdf-4.2.15 \t\t(where unrared HDF4 source code)\n",
    "\n",
    "Then:\n",
    "\n",
    "> export FCFLAGS=\"-w -fallow-argument-mismatch -O2\"\n",
    "\n",
    "> export FFLAGS=\"-w -fallow-argument-mismatch -O2\"\n",
    "\n",
    "> export FFLAGS=\"-w -fno-second-underscore -O2\"\n",
    "\n",
    "> ./configure --with-zlib=/usr --with-jpeg=/usr --disable-netcdf --prefix=/home/nilik/MYPROGRAMS/hdf4\n",
    "\n",
    "> gmake\n",
    "\n",
    "> gmake check\n",
    "\n",
    "> gmake install\n",
    "\n",
    "*After install it is important to check that is there “hdf.f90” in “/home/nilik/MYPROGRAMS/hdf4/include” directory or not? It should be created.*\n",
    "\n",
    "(https://www.programmersought.com/article/34565557036/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69aa44-ba1d-45fc-90eb-a564e6a18532",
   "metadata": {},
   "source": [
    "19- Install HDFEOS:\n",
    "\n",
    "download HDFEOS from (with VPN): https://wiki.earthdata.nasa.gov/display/DAS/Toolkit+Downloads\n",
    "\n",
    "https://git.earthdata.nasa.gov/rest/git-lfs/storage/DAS/hdfeos/cb0f900d2732ab01e51284d6c9e90d0e852d61bba9bce3b43af0430ab5414903?response-content-disposition=attachment%3B%20filename%3D%22HDF-EOS2.20v1.00.tar.Z%22%3B%20filename*%3Dutf-8%27%27HDF-EOS2.20v1.00.tar.Z\n",
    "\n",
    "\n",
    "> ./configure --prefix=/usr/local/hdfeos CC='/usr/lib/bin/h4cc -Df2cFortran' --enable-install_include\n",
    "\n",
    "> sudo make\n",
    "\n",
    "> sudo make install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5fef77-2287-4bfe-b330-6f06191cb370",
   "metadata": {},
   "source": [
    "## Install ESMF final method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a740079-6145-4f11-b7aa-8f10997db2f4",
   "metadata": {},
   "source": [
    "src:https://earthscience.stackexchange.com/questions/18758/how-to-install-esmf-and-esmfpy-in-ubuntu-using-gfortran-gcc-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f1d2b-1390-4a74-a781-1f3a3b2c1dd9",
   "metadata": {},
   "source": [
    "### Very important NOTE:\n",
    "\n",
    "For everytime to install new or refresh ESMF installation it is necessary to check below codes and then define the following environmental variables!\n",
    "\n",
    "> sudo apt-get install git tcsh pkg-config\n",
    "> sudo apt-get install gfortran\n",
    "> sudo apt-get install netcdf-bin libnetcdf-dev libnetcdff-dev\n",
    "> sudo apt-get install openmpi-bin libopenmpi-dev\n",
    "> sudo apt-get install libnetcdff-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a4ff7-73ea-42cf-a392-010fbc845de2",
   "metadata": {},
   "source": [
    "1- Download ESMF == http://earthsystemmodeling.org/download/\n",
    "\n",
    "2- Extract ESMF downloaded == /home/nilik/Temp/esmf-ESMF_8_1_1\n",
    "\n",
    "3- login as root using\n",
    "\n",
    "> sudo -i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2926e1-109f-4404-98cc-0ba5f92dc5cf",
   "metadata": {},
   "source": [
    "4- Define the following environment variables:\n",
    "\n",
    "> cd /home/nilik/Temp/esmf-ESMF_8_1_1\n",
    "\n",
    "> export ESMF_DIR=/home/nilik/Temp/esmf-ESMF_8_1_1\n",
    "\n",
    "> export ESMF_INSTALL_PREFIX=/home/nilik/MYPROGRAMS/LDT/ESMF\n",
    "\n",
    "> export ESMF_OS=Linux\n",
    "\n",
    "> export ESMF_NETCDF=/usr/include\n",
    "\n",
    "> export ESMF_COMM=mpiuni\n",
    "\n",
    "> export ESMF_NETCDF_INCLUDE=/usr/local/include\n",
    "\n",
    "> export ESMF_NETCDF_LIBS=\"-lnetcdf -lnetcdff\"\n",
    "\n",
    "> export ESMF_NETCDF_LIBPATH=/usr/local/lib\n",
    "\n",
    "\n",
    "5- Run the following syntax to make the library ESMF:\n",
    "\n",
    "> make all\n",
    "\n",
    "> make install\n",
    "\n",
    "> make installcheck\n",
    "\n",
    "> export ESMF_INC=$ESMF_INSTALL_PREFIX/include\n",
    "\n",
    "> export ESMF_LIB=$ESMF_INSTALL_PREFIX/lib/libO/Linux.intel.64.intelmpi.default\n",
    "\n",
    "> export ESMFMKFILE=$ESMF_INSTALL_PREFIX/lib/libO/Linux.intel.64.intelmpi.default/esmf.mk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b0785-f036-4033-9a6c-c8d90b3e666c",
   "metadata": {},
   "source": [
    "6- Install the ESMF python library:\n",
    "\n",
    "> cd /home/nilik/Temp/esmf-ESMF_8_1_1/src/addon/ESMPy\n",
    "\n",
    ">python3 setup.py build --ESMFMKFILE=/home/nilik/MYPROGRAMS/LDT/ESMF/lib/libO/Linux.gfortran.64.mpiuni.default/esmf.mk\n",
    "\n",
    "> sudo python3 setup.py install\n",
    "\n",
    "7- Check the installation by:\n",
    "\n",
    "> $ python\n",
    "\n",
    "> import ESMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd761ea2-9d35-4a66-a9c4-cc6e4ea2941d",
   "metadata": {},
   "source": [
    "## Compile LDT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f426bc3c-4022-4283-82f5-799ae883aa9b",
   "metadata": {},
   "source": [
    "*Note: If you compile LDT before for new build or new compile LDT you must run 'sudo make realclean' in the ldt/make directory before new running*\n",
    "\n",
    "1- Download LIS NASA from https://lis.gsfc.nasa.gov/ OR https://github.com/NASA-LIS/LISF/releases/tag/v7.3.1-public\n",
    "\n",
    "2- Unpack in TOPLEVELDIR\n",
    "\n",
    "3- Open terminal in \"/home/nilik/MYPROGRAMS/LDT/TOPLEVELDIR/ldt\" then:\n",
    "\n",
    "> export LDT_SRC=/home/nilik/MYPROGRAMS/LDT/TOPLEVELDIR/ldt\n",
    "\n",
    "> export LDT_ARCH=linux_gfortran\n",
    "\n",
    "> export LDT_FC=gfortran\n",
    "\n",
    "> export LDT_CC=gcc\n",
    "\n",
    "> export LDT_MODESMF=/home/nilik/MYPROGRAMS/LDT/ESMF/mod/modO/Linux.gfortran.64.mpiuni.default\n",
    "\n",
    "> export LDT_LIBESMF=/home/nilik/MYPROGRAMS/LDT/ESMF/lib/libO/Linux.gfortran.64.mpiuni.default\n",
    "\n",
    "> export LDT_NETCDF=/usr\n",
    "\n",
    "> export LDT_HDF5=/usr/lib/x86_64-linux-gnu/hdf5/serial\n",
    "\n",
    "> export LDT_HDF4=/home/nilik/MYPROGRAMS/hdf4\n",
    "\n",
    "> export LDT_HDFEOS=/usr/local/hdfeos\n",
    "\n",
    "> export LDT_ECCODES=/usr/local\n",
    "\n",
    "> export LDT_OPENJPEG=/usr\n",
    "\n",
    "> ./configure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61908723-7f14-489f-950d-33629a7def96",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "Before run compile add *\"-lrt\"* to the *LDFLAGS* line of make/configure.ldt and then compile.\n",
    "\n",
    "AND \n",
    "\n",
    "For prevent errors when compile such as: \"Error: Type mismatch between actual argument at (1) and actual argument at (2)...\" etc.\n",
    "\n",
    "It is neccessary to add \n",
    "```\n",
    "-finit-local-zero -fno-strict-overflow -fallow-argument-mismatch -fallow-invalid-boz \n",
    "``` \n",
    "in *\"FFLAGS\"* of configure file.\n",
    "\n",
    "AND\n",
    "\n",
    "add *“-ltirpc”* to *LDFLAGS* for activate XDR in LDT compile.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "\n",
    "FFLAGS          =  -c -pass-exit-codes -ffree-line-length-0 -O2   -fconvert=big-endian -DGFORTRAN -I$(MOD_ESMF)  -I$(INC_ECCODES) -I$(INC_NETCDF)  -I$(INC_HDFEOS)  -I$(INC_HDF4)  -I$(INC_HDF5) -finit-local-zero -fno-strict-overflow -fallow-argument-mismatch -fallow-invalid-boz\n",
    "\n",
    "LDFLAGS         =  -L$(LIB_ESMF) -lesmf -lstdc++ -lrt -lz -L$(LIB_ECCODES) -leccodes_f90 -leccodes -L$(LIB_JPEG2000) -lopenjp2 -L$(LIB_NETCDF) -lnetcdff -lnetcdf -L$(LIB_HDF5) -lhdf5_fortran -lhdf5_hl -lhdf5 -ldl -ltirpc\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed2330-b92c-4325-b772-01ada5ee5ab2",
   "metadata": {},
   "source": [
    "4- \n",
    "> ./compile\n",
    "\n",
    "OR\n",
    "\n",
    "> ./compile >make.log 2>&1            (for create a log file)\n",
    "\n",
    "\n",
    "*Note: For error of \"/usr/bin/env: ‘python’: No such file or directory\" when run compile, install python3 with:*\n",
    "\n",
    "> sudo apt-get install python-is-python3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ff008-4cc8-4d93-a650-863548a60f6f",
   "metadata": {},
   "source": [
    "__NOTE: After Compiled Finished \"LDT\" file created in ldt directory.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10f99f-7b08-4b0b-9055-f4ba83e2d21c",
   "metadata": {},
   "source": [
    "## Using LDT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983d087-5c73-42d2-9147-0d0646b49a7f",
   "metadata": {},
   "source": [
    "For use LDT executable file that created in the previous steps, It is best to test LDT with LIS tutorials. \n",
    "\n",
    "So, for do that the \"testcase1_ldt_parms_2020\" is best for start.\n",
    "Base on \"LDT_Parameters_Testcase_Step1.pdf\" of LIS Testcases I did below steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74503f8f-cecf-4d41-af32-8623b1ddd638",
   "metadata": {},
   "source": [
    "1- Build a new folder as name as \"RUNNING\" in \"/home/nilik/Temp\"\n",
    "\n",
    "2- Copy contents of \"testcase1_ldt_parms_2020\" directory to RUNNING.\n",
    "\n",
    "3- Copy LDT executable file created to RUNNING.\n",
    "\n",
    "4- Open terminal in this folder\n",
    "\n",
    "5- type: \":~/RUNNING$ ulimit -s unlimited\"\n",
    "\n",
    "6- type: \"$ ./LDT ldt.config.noah36_params\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d97e02-75c6-4ff5-abf8-3782b2a76022",
   "metadata": {},
   "source": [
    "7- ERROR:\n",
    "\n",
    "\"./LDT: error while loading shared libraries: libesmf.so: cannot open shared object file: No such file or directory\"\n",
    "\n",
    "*For solve error you must define corrct path for \"libesmf.so\", so:*\n",
    "```\n",
    "\":~/RUNNING$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/nilik/MYPROGRAMS/LDT/ESMF/lib/libO/Linux.gfortran.64.mpiuni.default\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f67f53-8aa7-4c0b-9834-1ecac02433b1",
   "metadata": {},
   "source": [
    "8- Again run:\n",
    "```\n",
    "\"$ ./LDT ldt.config.noah36_params\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7429442e-f2fb-47ad-8389-9911c980c385",
   "metadata": {},
   "source": [
    "9- ERROR:\n",
    "\n",
    "```\n",
    "./LDT: error while loading shared libraries: libeccodes_f90.so: cannot open shared object file: No such file or directory\n",
    "```\n",
    "\n",
    "For solve error you must define correct path for \"libeccodes_f90.so\", so:\n",
    "\n",
    "```\n",
    ":~/RUNNING$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/hm/ecCodes/lib\n",
    "```\n",
    "\n",
    "In sum:\n",
    "\n",
    "```\n",
    ":~/RUNNING$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/hm/esmf/lib/libO/Linux.gfortran.64.mpiuni.default\n",
    ":~/RUNNING$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/hm/ecCodes/lib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ab0016-fb6d-4c3b-8669-525905f335cf",
   "metadata": {},
   "source": [
    "10- Again run: \n",
    "\n",
    "```\n",
    "$ ./LDT ldt.config.noah36_params\n",
    "\n",
    "```\n",
    "\n",
    "11- After a little time the program stopped without any error message while two new files include __ldtlog.0000__ and __MaskParamFill.log__ created. It seems that there is a problem. So, with comparison of __MaskParamFill.log__ created and __target_MaskParamFill.log__ in \"target_log\" directory it can be know the problem.\n",
    "\n",
    "12- In __MaskParamFill.log__ file created there are only 16 lines but in \"target_MaskParamFill.log\" file there are more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001db51-e914-4799-a462-432df0ba43e8",
   "metadata": {},
   "source": [
    "13- In line 17 of __target_MaskParamFill.log__ file:\n",
    "\n",
    "```\n",
    " Checking/filling mask values for: MXSNALBEDO\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ce2ba-2a1b-4bd8-91fc-6c6b565ee1fd",
   "metadata": {},
   "source": [
    "It seems that the problem may related to Max Snow Albedo. In addition based on answer the quesion in \"https://modelingguru.nasa.gov/message/10861#10861\", this problem can be related to __HDF4__. In previous steps I didn't use HDF4 for LDT compilation. \n",
    " \"HDF4 is required to read the max snow albedo dataset for the first step of the public testcases.\"\n",
    "\n",
    "I reinstalled HDF4 again correctly and run LDT correctly :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23488743-94bf-4e98-81fb-5f40f1295a7f",
   "metadata": {},
   "source": [
    "AFTER INSTALL CORRECTLY LIBRARIES AGAIN in UBUNTU-BUDGIE,\n",
    "I CAN RUN FIRST TESTCASE WITH NAME of *testcase1_ldt_parms_2020*.\n",
    "\n",
    "NOW I SHOULD USE LDT FOR MY STUDY AREA AND MERRA-2 DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bdc307-4c1f-42d1-805e-c0fe24547f0b",
   "metadata": {},
   "source": [
    "## More about LDT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de288d5-5f65-40cf-a25b-1428fe4d7037",
   "metadata": {},
   "source": [
    "### Native Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf144d3e-d6e5-403d-813d-728f31b8de19",
   "metadata": {},
   "source": [
    "The Native Model Parameters for LDT.\n",
    "The \"Native\" model parameters and data sets are obtained from different data providers, including data centers (e.g., NASA's GES DISC), government and university partners (e.g., NOAA websites).\n",
    "To help users become more familiar with the use of these datasets, several examples are provided in our latest test case suite. These use cases should be helpful in learning how to run LDT and process the \"native\" parameters onto a target grid and region of interest.\n",
    "\n",
    "Some examples of \"native\" parameters and data include:\n",
    "\n",
    "    • MODIS-IGBP landcover \n",
    "    • STATSGO+FAO soil texture \n",
    "    • SRTM topographic maps (e.g., elevation, slope) \n",
    "    • and many other model inputs (e.g., different Noah land model versions) \n",
    "    \n",
    "(src: https://lis.gsfc.nasa.gov/data/ldt/native)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88f8ed-e4a1-4ce9-abcf-010117095e58",
   "metadata": {},
   "source": [
    "## Land surface Data Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1692e6f-213d-4287-ac45-d9d1af950b41",
   "metadata": {},
   "source": [
    "The Land surface Data Toolkit (LDT) is the front-end processor for the Land Information System (LIS), versions 7 and greater.  It provides an environment for processing land model data and parameters, as well as restart files and data assimilation based inputs (e.g., for bias correction methods), and other data inputs for LIS and LVT (Arsenault et al., 2018).\n",
    "\n",
    "LDT also offers a variety of inputs and user options to process datasets and is designed with not only LIS in mind, but for other independent model and modeling systems as well. LDT supports the use of common data formats, like NetCDF, which provide detailed data header information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcaf929-762f-4958-af5a-644075d26756",
   "metadata": {},
   "source": [
    "<center><img src='images/ldt.png' width=\"500\" height=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aa3caf-b356-4720-a22d-00c47c47bb42",
   "metadata": {},
   "source": [
    "__Some Major Features__\n",
    "\n",
    "    • Processes data inputs for land surface, hydrological, and lake models. \n",
    "    • Writes output in NetCDF, a common descriptive format. \n",
    "    • Supports multiple observational data sources. \n",
    "    • Offers a variety of projections and grid transformation options. \n",
    "    • Includes numerous options for processing parameters (e.g., agreement between parameters). \n",
    "    • Can function as a stand-alone land surface and data assimilation input processor in addition to being a pre-processor for LIS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608e24d-e881-425f-9ded-79f549beaa78",
   "metadata": {},
   "source": [
    "__New Data Options__\n",
    "\n",
    "LDT is designed to read \"native\" model parameters and data files available from their original sources, as well as being backward-compatible with the original LIS-generated binary-formatted parameter datasets. Please note that the LIS team is now moving away from these older LIS binary-formatted files.\n",
    "\n",
    "\n",
    "__Philosophy__\n",
    "\n",
    "LDT has been designed and developed to read in what are considered the \"native\" or raw original data files as how they are provided by a data center, government agency, university group, etc.\n",
    "This philosophy that data and model parameter files should be read in from their \"native\" grids and file formats and be written to a common descriptive data format, like NetCDF, is supported throughout all of the LIS framework software.\n",
    "\n",
    "__Capabilities__\n",
    "\n",
    "LDT's Functional Goals involve developing certain key features, which include:\n",
    "    • Process surface model (e.g., land surface models) parameters onto a common grid domain. \n",
    "    • Generate model initial conditions (e.g., model ensemble initialization). \n",
    "    • Generate CDF statistics that can be used in LIS during data assimilation updates. \n",
    "    • Implement and apply quality control measures to parameters, independent validation datasets, etc. \n",
    "    • Process and temporally downscale meteorological forcing datasets. \n",
    "    • Support for processing observations for data assimilation procedures in LIS. \n",
    "    • Machine learning layer being developed and expanded for more applications.\n",
    "    \n",
    "(src: https://lis.gsfc.nasa.gov/software/ldt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747ca27-998c-45a2-aff1-65a0fb932db1",
   "metadata": {},
   "source": [
    "*What land surface models (LSMs) are currently supported within the LIS framework?*\n",
    "\n",
    "The current public versions of LIS support the following LSMs:\n",
    "\n",
    "    • Noah 2.7.1 \n",
    "    • Noah 3.2 \n",
    "    • Noah 3.3 \n",
    "    • Noah 3.6 \n",
    "    • Noah 3.9 \n",
    "    • Noah Multi-Physics (MP), version 3.6 \n",
    "    • Catchment LSM (CLSM), Fortuna 2.5 version \n",
    "    • VIC 4.1.1 \n",
    "    • VIC 4.1.2 \n",
    "    • SAC-HTET/SNOW-17 \n",
    "    • GeoWRSI 2.0 \n",
    "    • CLM 2.0 \n",
    "    • Mosaic \n",
    "    • HY-SSib \n",
    "    \n",
    "*What projections are currently supported in LDT and LIS-7?*\n",
    "\n",
    "Currently, several map projections are supported for both input parameters and the LIS target run grid. These include: 'latlon', 'lambert' (for lambert conformal), 'polar' (for polar stereographic), 'hrap' (for HRAP, a flavor of polar stereographic), 'mercator', and 'gaussian' (which is supported in LDT and will be for LIS-7 in future releases). Other projections, like 'UTM', will also be supported in future releases.\n",
    "\n",
    "*What differences are involved with a \"readin\" landmask vs. wanting to \"create\" one?*\n",
    "\n",
    "With LDT, LIS users can either \"readin\" an existing land-water mask or \"create\" one from the landcover map that is read in (e.g., a landmask derived from MODIS-IGBP, ESA). For most of LIS' history, most users were restricted to using just the \"UMD\" landmask, which came from the AVHRR satellite-based UMD classification map, circa 1992-1993. Today, LIS users have several different landcover datasets and classifications they can chose from, and from which they can create landmasks. Also, any other landmask can be read in, like the MODIS-based MOD44w landmask product, which is now used in the MODIS Collection 6 products.\n",
    " \n",
    "Because reading in or creating a landmask map may not share matching land points with other parameter maps read in (e.g., soil texture, elevation), \"fill\" options are provided in LDT to help ensure agreement  between the mask and the parameter fields. These \"fill\" routines are based on earlier code used to impose the \"UMD\" landmask on all of the original LIS-team produced binary files (aka, the \"LIS-data\" files). Also, the \"fill\" step occurs on the LIS output run domain after the spatial transform step is performed.\n",
    "\n",
    "*What are the 'fill' options (e.g., fill radius)?*\n",
    "\n",
    "The 'fill' options associated with each parameter entry in the LDT config file provide the user the ability to make sure the read-in or derived landmask and the read-in parameter files agree (i.e., same number of land points). Continuous type parameter files can utilize either 'neighbor' or 'average' options in using neighboring pixels to 'fill' in a missing parameter value when there is an actual valid land point (i.e., landmask = 1). Discrete data types can only use 'neighbor' option at this time (e.g., landcover, soil texture, etc.). Additional options include the 'fill radius' which the user can enter the number of row/column pixel area to search for neighboring valid parameter values. If no neighboring values are found (e.g., could be an island), the user can specify a 'generic fill' value for that given parameter (e.g., input the land class of 6 for the landcover parameter).\n",
    "\n",
    "*What is a 'spatial tranform' and when is it applied?*\n",
    "\n",
    "A 'spatial transform' entry in the run-time LDT configure (e.g., ldt.config) file allows the user to specify how a parameter file can be 'transformed' or translated from its 'native' or original projection and resolution to the designated LIS target grid projection and resolution.\n",
    " \n",
    "For example, you want to upscale your 0.01°, lat-lon grid landcover file to a 0.25°, lat-lon grid map. You can select 'tile' as the entry for the 'Landcover spatial transform:' option, and LDT will aggregate the landcover pixels to tile layers (i.e., fraction of each landcover type) within each 0.25° output gridcell. If you were to select 'mode' instead here, a value of \"1\" would be assigned to the most dominant vegetation layer, representing 100% coverage. Since landcover is a discrete data type, you would not want to select interpolation methods, like 'bilinear', or upscaling option, 'average' (or you would end up with a landcover type of 4.331!).\n",
    " \n",
    "For a downscaling example, you could have a 1.0°, lat-lon grid albedo map and you want to run on a 0.25°, lat-lon output grid. You could select 'bilinear' for interpolating the coarser albedo parameter map to the finer scale 0.25° map.\n",
    "\n",
    "*What is the 'LIS' data format type?*\n",
    "\n",
    "The other type of data read in by LDT includes the LIS team processed files referred to commonly as 'LIS' data. These original files are 4-byte real, big-endian, direct access, binary format, with the extension: *.1gd4r . One downside to using these datasets is that many of them have the \"UMD\" landmask imposed on all the parameter files, forcing users to use this older mask in their model runs. LDT, LIS and LVT are moving away from this data formatted file use.\n",
    "\n",
    "*What are the 'Native' data files?*\n",
    "\n",
    "When LDT was being developed, the LIS team decided to provide two pathways for the LIS user community for how data parameters can be read in to LDT and LIS-7. The first being that any original or 'native' model parameters and data provided by a government agency, university, organization, etc. should be read in \"as-is\" and not have any preprocessing done to the files. This \"philosophy\" is intended to better preserve the original data information and pixel integrity but also allow the user to select how the parameters get processed for their modeling needs.\n",
    "\n",
    "*How does topographic downscaling of the bottom temperature field work?*\n",
    "\n",
    "To capture some of the impact of terrain relief on bottom soil temperature field variability, you can turn the 'Bottom temperature topographic downscaling:' entry in the ldt.config file to \"lapse-rate\". You will also need to also turn on and read in an elevation map, preferably of higher resolution than your bottom temperature map. Then using the elevation file and the environmental lapse rate (~6.5 K/km), the bottom temperature file values are adjusted by the elevation values, representing what the soil temperature might reflect at a higher or lower elevation. This feature helps then give more detail in mountainous areas and capture local minimum temperatures in the soil temperatures than if not accounted for.\n",
    "\n",
    "*What type of data assimilation inputs does LDT process?*\n",
    "\n",
    "Currently, LDT can estimate the statistics required to do a simple bias-correction or scaling approach between similar observational data and model state estimates to reduce bias between the two during assimilation update step. LDT generates the mean, standard deviation and cumulative (probability) density function (CDF) values, which LIS-7 ingests to perform the final CDF \"matching\" between the observations and the model estimates.\n",
    "\n",
    "*What LIS DA observation dataset types are supported in LDT and LIS?*\n",
    "\n",
    "The current public LIS versions support the following data assimilation (DA) observation dataset types:\n",
    "\n",
    "    • Soil Moisture Active Passive (SMAP) soil moisture products \n",
    "    • NASA/Vrije U. Land Parameter Retrieval Model (LPRM) AMSR-e Soil Moisture data \n",
    "    • NASA/NSIDC AMSR-e Soil Moisture data \n",
    "    • Essential Climate Variable (ECV) Satellite-based Soil Moisture analysis \n",
    "    • TU Wien ASCAT Soil Moisture (retrospective) \n",
    "    • NESDIS/OSPO Soil Moisture Operational Products System (SMOPS) Soil Moisture product \n",
    "    • WindSat satellite-based Soil Moisture Retrievals \n",
    "    • GRACE Terrestrial Water Storage (TWS) Estimates \n",
    "    • Synthetic Model-derived Soil Moisture output\n",
    "    \n",
    "*What is an ensemble restart file?*\n",
    "\n",
    "An ensemble restart file includes not only one single realization's set of model state variables required to initialize a LIS model run but several realizations, or ensemble members, to restart a multi-member or open-loop simulation.\n",
    "\n",
    "*What is meant by upscaling or downscaling a restart file?*\n",
    "\n",
    "Upscaling: refers to expanding a single-member (or realization) LIS-generated restart file to an ensemble of members (or model realizations) that can be used for ensemble model runs. This feature could be helpful to someone interested in performing a simulation of an ensemble of model realizations or data assimilation, but have only model member run for one single realization.\n",
    "\n",
    "__Downscaling__: refers to averaging or collapsing a multi-member (or -realization) LIS-generated restart file to a single member (or model realization), making it essentially a climatological realization of the member (ensemble mean simulation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034ffdf-87fe-4558-a21d-842598dbc0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
