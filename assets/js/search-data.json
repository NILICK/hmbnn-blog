{
  
    
        "post0": {
            "title": "Analysis process of downscaled MERRA-2 data",
            "content": "Introduction . In this post, I would like to describe full story of my experiences in spatial downscaling of MERRA-2 data. The main spark of using MERRA-2 data refers to the paper of Assessment of drought conditions over Vietnam using standardized precipitation evapotranspiration index, MERRA-2 re-analysis, and dynamic land cover (Manh-Hung Le et al., 2020). Researchers of this paper used spatial rescaled MERRA-2 data from ~50-km to ~1-km and used it for calculation of drought index. . In first step, I tried to use Land surface Data Toolkit (LDT). So, based on LDT User Guide, required libraries were installed for setup LDT in personal laptop. Although successfully installation of all necessary libraries in a hard way, but I didn&#39;t success in using LDT to spatial downscaling of MERRA-2 data. . After unsuccessful attempts to use LDT correctly, connection with authors of mentioned paper was seemed a right way. Dr. Hyunglok Kim as one of the paper&#39;s researches warned me that their spatial downscaled of MERRA-2 data was generated from the Land Information System (LIS), not LDT. Specifically, the LDT will just produce some basic information to produce data from LIS. Based on Hyung explanation about producing spatial rescaled data, me seemed to need a supercomputer to do that. With the help of Hyung, temperature and rain monthly data from 2000 to 2020 produced in 0.001 degree (~1 km) of spatial resolution. In the following, the procedure of analysis of this data will describe. . Overall view of data . Spatial downscaled of MERRA-2 data files was in NetCDF format. So, me preferred to use Xarray python library to check some its properties. . Opening 252 NetCDF files simultaneously will done with some python libraries. . # Import libraries import xarray as xr import matplotlib.pyplot as plt import numpy as np import pandas as pd %matplotlib inline plt.rcParams[&#39;figure.figsize&#39;] = (8,5) . . Reading files: . print(ds) . &lt;xarray.Dataset&gt; Dimensions: (time: 252, north_south: 701, east_west: 1001, ensemble: 1) Coordinates: * time (time) datetime64[ns] 2000-01-01 2000-02-01 ... 2020-12-01 * ensemble (ensemble) float32 1.0 Dimensions without coordinates: north_south, east_west Data variables: lat (time, north_south, east_west) float32 dask.array&lt;chunksize=(1, 701, 1001), meta=np.ndarray&gt; lon (time, north_south, east_west) float32 dask.array&lt;chunksize=(1, 701, 1001), meta=np.ndarray&gt; Rainf_f_tavg (time, ensemble, north_south, east_west) float32 dask.array&lt;chunksize=(1, 1, 701, 1001), meta=np.ndarray&gt; Rainf_f_inst (time, ensemble, north_south, east_west) float32 dask.array&lt;chunksize=(1, 1, 701, 1001), meta=np.ndarray&gt; Tair_f_tavg (time, ensemble, north_south, east_west) float32 dask.array&lt;chunksize=(1, 1, 701, 1001), meta=np.ndarray&gt; Tair_f_inst (time, ensemble, north_south, east_west) float32 dask.array&lt;chunksize=(1, 1, 701, 1001), meta=np.ndarray&gt; TotalPrecip_tavg (time, ensemble, north_south, east_west) float32 dask.array&lt;chunksize=(1, 1, 701, 1001), meta=np.ndarray&gt; TotalPrecip_inst (time, ensemble, north_south, east_west) float32 dask.array&lt;chunksize=(1, 1, 701, 1001), meta=np.ndarray&gt; Attributes: (12/15) missing_value: -9999.0 NUM_SOIL_LAYERS: 4 SOIL_LAYER_THICKNESSES: [0.1 0.3 0.6 1. ] title: LIS land surface model output institution: NASA GSFC source: +template open water ... ... comment: website: http://lis.gsfc.nasa.gov/ MAP_PROJECTION: EQUIDISTANT CYLINDRICAL SOUTH_WEST_CORNER_LAT: 27.0 SOUTH_WEST_CORNER_LON: 47.0 DX: 0.01 DY: 0.01 . . Output reading fils show that thera are time, north_south, east_west, ensemble as Dimensions, time and ensemble as Coordinates and lat, lon, Rainf_f_tavg, Rainf_f_inst, Tair_f_tavg, Tair_f_inst, TotalPrecip_tavg, TotalPrecip_inst as Data variables of data. . You can see all of these items individually as follow: . ds.dims . Frozen({&#39;time&#39;: 252, &#39;north_south&#39;: 701, &#39;east_west&#39;: 1001, &#39;ensemble&#39;: 1}) . . ds.coords . Coordinates: * time (time) datetime64[ns] 2000-01-01 2000-02-01 ... 2020-12-01 * ensemble (ensemble) float32 1.0 . . ds.variables . Frozen({&#39;lat&#39;: &lt;xarray.Variable (time: 252, north_south: 701, east_west: 1001)&gt; dask.array&lt;concatenate, shape=(252, 701, 1001), dtype=float32, chunksize=(1, 701, 1001), chunktype=numpy.ndarray&gt; Attributes: units: degree_north standard_name: latitude long_name: latitude vmin: 0.0 vmax: 0.0, &#39;lon&#39;: &lt;xarray.Variable (time: 252, north_south: 701, east_west: 1001)&gt; dask.array&lt;concatenate, shape=(252, 701, 1001), dtype=float32, chunksize=(1, 701, 1001), chunktype=numpy.ndarray&gt; Attributes: units: degree_east standard_name: longitude long_name: longitude vmin: 0.0 vmax: 0.0, &#39;time&#39;: &lt;xarray.IndexVariable &#39;time&#39; (time: 252)&gt; array([&#39;2000-01-01T00:00:00.000000000&#39;, &#39;2000-02-01T00:00:00.000000000&#39;, &#39;2000-03-01T00:00:00.000000000&#39;, ..., &#39;2020-10-01T00:00:00.000000000&#39;, &#39;2020-11-01T00:00:00.000000000&#39;, &#39;2020-12-01T00:00:00.000000000&#39;], dtype=&#39;datetime64[ns]&#39;) Attributes: long_name: time time_increment: 2592000 begin_date: 20000101 begin_time: 000000, &#39;ensemble&#39;: &lt;xarray.IndexVariable &#39;ensemble&#39; (ensemble: 1)&gt; array([1.], dtype=float32) Attributes: units: ensemble number long_name: Ensemble numbers, &#39;Rainf_f_tavg&#39;: &lt;xarray.Variable (time: 252, ensemble: 1, north_south: 701, east_west: 1001)&gt; dask.array&lt;concatenate, shape=(252, 1, 701, 1001), dtype=float32, chunksize=(1, 1, 701, 1001), chunktype=numpy.ndarray&gt; Attributes: units: kg m-2 s-1 standard_name: rainfall_flux long_name: rainfall flux vmin: 0.0 vmax: 0.02, &#39;Rainf_f_inst&#39;: &lt;xarray.Variable (time: 252, ensemble: 1, north_south: 701, east_west: 1001)&gt; dask.array&lt;concatenate, shape=(252, 1, 701, 1001), dtype=float32, chunksize=(1, 1, 701, 1001), chunktype=numpy.ndarray&gt; Attributes: units: kg m-2 s-1 standard_name: rainfall_flux long_name: rainfall flux vmin: 0.0 vmax: 0.02, &#39;Tair_f_tavg&#39;: &lt;xarray.Variable (time: 252, ensemble: 1, north_south: 701, east_west: 1001)&gt; dask.array&lt;concatenate, shape=(252, 1, 701, 1001), dtype=float32, chunksize=(1, 1, 701, 1001), chunktype=numpy.ndarray&gt; Attributes: units: K standard_name: air_temperature long_name: air temperature vmin: 213.0 vmax: 333.0, &#39;Tair_f_inst&#39;: &lt;xarray.Variable (time: 252, ensemble: 1, north_south: 701, east_west: 1001)&gt; dask.array&lt;concatenate, shape=(252, 1, 701, 1001), dtype=float32, chunksize=(1, 1, 701, 1001), chunktype=numpy.ndarray&gt; Attributes: units: K standard_name: air_temperature long_name: air temperature vmin: 213.0 vmax: 333.0, &#39;TotalPrecip_tavg&#39;: &lt;xarray.Variable (time: 252, ensemble: 1, north_south: 701, east_west: 1001)&gt; dask.array&lt;concatenate, shape=(252, 1, 701, 1001), dtype=float32, chunksize=(1, 1, 701, 1001), chunktype=numpy.ndarray&gt; Attributes: units: kg m-2 s-1 standard_name: total_precipitation_amount long_name: total precipitation amount vmin: 0.0 vmax: 0.02, &#39;TotalPrecip_inst&#39;: &lt;xarray.Variable (time: 252, ensemble: 1, north_south: 701, east_west: 1001)&gt; dask.array&lt;concatenate, shape=(252, 1, 701, 1001), dtype=float32, chunksize=(1, 1, 701, 1001), chunktype=numpy.ndarray&gt; Attributes: units: kg m-2 s-1 standard_name: total_precipitation_amount long_name: total precipitation amount vmin: 0.0 vmax: 0.02}) . . ds.attrs . {&#39;missing_value&#39;: -9999.0, &#39;NUM_SOIL_LAYERS&#39;: 4, &#39;SOIL_LAYER_THICKNESSES&#39;: array([0.1, 0.3, 0.6, 1. ], dtype=float32), &#39;title&#39;: &#39;LIS land surface model output&#39;, &#39;institution&#39;: &#39;NASA GSFC&#39;, &#39;source&#39;: &#39;+template open water&#39;, &#39;history&#39;: &#39;created on date: 2021-07-10T04:31:27.879&#39;, &#39;references&#39;: &#39;Kumar_etal_EMS_2006, Peters-Lidard_etal_ISSE_2007&#39;, &#39;conventions&#39;: &#39;CF-1.6&#39;, &#39;comment&#39;: &#39;website: http://lis.gsfc.nasa.gov/&#39;, &#39;MAP_PROJECTION&#39;: &#39;EQUIDISTANT CYLINDRICAL&#39;, &#39;SOUTH_WEST_CORNER_LAT&#39;: 27.0, &#39;SOUTH_WEST_CORNER_LON&#39;: 47.0, &#39;DX&#39;: 0.01, &#39;DY&#39;: 0.01} . . It is possible to show items for a dimension of our dataset that can be useful in future analysis. . print(ds.time) . &lt;xarray.DataArray &#39;time&#39; (time: 252)&gt; array([&#39;2000-01-01T00:00:00.000000000&#39;, &#39;2000-02-01T00:00:00.000000000&#39;, &#39;2000-03-01T00:00:00.000000000&#39;, ..., &#39;2020-10-01T00:00:00.000000000&#39;, &#39;2020-11-01T00:00:00.000000000&#39;, &#39;2020-12-01T00:00:00.000000000&#39;], dtype=&#39;datetime64[ns]&#39;) Coordinates: * time (time) datetime64[ns] 2000-01-01 2000-02-01 ... 2020-12-01 Attributes: long_name: time time_increment: 2592000 begin_date: 20000101 begin_time: 000000 . . Simple Plotting . Xarray library in combination of matplotlib library has powerful capability to draw various plots. For example in section will show some plots with xarray. Dataset variables show there are three climate variables include air temperature, rainfall and total precipitation. We can plot air temperature in first time as follow. . temp = ds.Tair_f_tavg temp[0].plot(); . . There is another way to plot based on specific time. . temp = ds.Tair_f_tavg temp.sel(time=&#39;2000-01-01&#39;).plot(); . Despite there are lat and lon data variables in dataset, in above plot geographical coordinates have been displayed incorrectly. Because lat and lon are not as dimension, we can not display geographical coordinates correctly. For improve the plot in showing geographical coordinates, mentioned data variables should be set as coords. Furthermore, air temperature unit is in kelvin [K], while we need it as celsius [C]. So it is necessary to convert air temperature unit. . # set `lat` and `lon` as coords ds = ds.set_coords([&#39;lon&#39;,&#39;lat&#39;]) # convert `air temperature` unit from kelin to celsius temp_c = ds.Tair_f_tavg - 273.15 # plotting `air temperature` data variable in celsius and geographical coordinates. temp_c.sel(time=&#39;2000-01-01&#39;).plot(x=&#39;lon&#39;, y=&#39;lat&#39;); . . Simple statistics . Now, with new dataarray created for air temperature it is possible to compute some statistics for this climate variable such as mean, std, min, max, etc. For example, some air temperature statistics for August 2015 are shown as follow. . temp_20150825 = temp_c.sel(time=&#39;2015-08-01&#39;) temp_avg = temp_20150825.mean().values temp_std = temp_20150825.std().values temp_min = temp_20150825.min().values temp_max = temp_20150825.max().values print(&quot;Mean air temperature for August 2015 is&quot;, temp_avg , &quot;celsius. n&quot; &quot;Standard deviation of air temperature for August 2015 is&quot;, temp_std ,&quot;. n&quot; &quot;Minimum air temperature for August 2015 is&quot;, temp_min , &quot;celsius. n&quot; &quot;Maximum air temperature for August 2015 is&quot;, temp_max , &quot;celsius.&quot;) . . Mean air temperature for August 2015 is 31.591408 celsius. Standard deviation of air temperature for August 2015 is 4.980299 . Minimum air temperature for August 2015 is 21.36026 celsius. Maximum air temperature for August 2015 is 40.377167 celsius. . . Tip: Our data is monthly from 2000 to 2020.. . Certain point values . The Merre-2 rescaled data is a reanalysis data with contiguous values, so if we want to compare this values with climatic station data as ground values it is important to extract the climate variables from Merra-2 dataset in a specific values in accordance with climatic station latitude and longitude. For extract this value(s) based on certain latitude and longitude we can finding the index of the grid point nearest a specific lat/lon. . In this example we want to find air temperature in latitude = 30.66 and longitude = 51.58 for August 2015. . latitude = 30.66 longitude = 51.58 #finding the index of the grid point nearest a specific lat/lon. abslat = np.abs(temp_20150825.lat-latitude) abslon = np.abs(temp_20150825.lon-longitude) c = np.maximum(abslon, abslat) ([xloc], [yloc]) = np.where(c == np.min(c)) # Using that index location to get the values at the x/y diminsion point_ds = ds.sel(north_south=xloc, east_west=yloc) # Value of certain point in celsius temp_pnt = point_ds.sel(time=&#39;2015-08-01&#39;).Tair_f_tavg.values- 273.15 print(&quot;The monthly air temperature in August 2015 for latitude=&quot;, latitude, &quot; and longitude=&quot;, longitude, &quot;is&quot;, format(float(temp_pnt), &#39;.2f&#39;), &quot;celsius.&quot;) . . The monthly air temperature in August 2015 for latitude= 30.66 and longitude= 51.58 is 25.74 celsius. . We can also plot for this point in August 2015. . # Plot requested lat/lon point blue temp_20150825.plot(x=&#39;lon&#39;, y=&#39;lat&#39;) plt.scatter(longitude, latitude, color=&#39;b&#39;) plt.text(longitude, latitude, &#39;requested&#39;) # Plot nearest point in the array red plt.scatter(point_ds.lon[0], point_ds.lat[0], color=&#39;r&#39;) plt.text(point_ds.lon[0], point_ds.lat[0], &#39;nearest&#39;) plt.title(&#39;Temperature at nearest point: %s celsius&#39; % format(float(temp_pnt), &#39;.2f&#39;)); . . The mean monthly temperature for all time series can also be calculated. . # Mean temperature average for specifiv lat/lon pnt = point_ds.Tair_f_tavg.values - 273.15 print(&quot;The monthly air temperature from 2000 to 2020 for latitude=&quot;, latitude, &quot; and longitude=&quot;, longitude, &quot;is&quot;, format(float(pnt.mean()), &#39;.2f&#39;), &quot;celsius.&quot;) . . The monthly air temperature from 2000 to 2020 for latitude= 30.667 and longitude= 51.583 is 14.65 celsius. . Swap &quot;north_south&quot; and &quot;east_west&quot; dimensions with &quot;lat&quot; and &quot;lon&quot; coordinates . One of the perbolem in working with this dataset is about latitude/longitude. As you saw before there is not any dimension related latitude and longitude and we used set_coords to define lat and lot data variables to plot correctly in geographical coordinates while dataset did not any change in inherent and do any analysis related to latitude and longitude of dataset will have challenges. So, it&#39;s best suggestion to create new dataset with raw data that have latitude and longitude dimensions. Two dimensions include north_south and east_west are linespaces related to latitude and longitude. In this dimensions, from beginning to ending latitude divide based on 0.001 degree distances that first value for north_south set to zero and last value will be based on dividing distances. For example for this dataset zero value of north_south is equal minimum latitude (27.0 degree) and last value (700) is equal maximum latitude (34.0 degree). The east_wet dimension values is also same as north_south but for longitude. So, if we can swap the north_south and east_west dimensions values with original latitude and longitude, respectively, we will have new dataset with original latitude and longitude dimensions. . For swapping &quot;north_south&quot; and &quot;east_west&quot; dimensions with lat and lon coordinates there is a Regridder function in xesmf library. Because of multiple NetCDF files should be regridded, so it&#39;s better to done it in a loop. . Although input netcdf files have small size but output netcdf files exported from xarray dataset will be more than x9-10 larger, so for solve this challenge we use zarr library to save output regridded files. But if drop unuseful data variables such as &#39;Rainf_f_inst&#39;, &#39;Tair_f_inst&#39;, &#39;TotalPrecip_inst&#39; and set remain data variables data type as float32, we can save output file in netcdf format with smaller size. . import numpy as np import xarray as xr import matplotlib.pyplot as plt import xesmf as xe import zarr import warnings warnings.filterwarnings(&#39;ignore&#39;) # reading dataset ds = xr.open_mfdataset(&quot;/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/monthly_data/*.nc&quot;) ds = ds.set_coords([&#39;lon&#39;,&#39;lat&#39;]) # definign outout path path_output = &#39;/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/monthly_data/monthly_regrid/&#39; # regriding loop for i in range (0, len(ds.time)): dsi = ds.sel(time = ds.time[i]) # swap dimensions with multi-dimensional coordinates ds_xy_grid = dsi.rename(north_south=&#39;lat&#39;, east_west=&#39;lon&#39;) ds_out = xr.Dataset({&#39;lat&#39;: ([&#39;lat&#39;], np.linspace(27.0, 34.0, 701)), &#39;lon&#39;: ([&#39;lon&#39;], np.linspace(47.0, 57.0, 1001))}) regridder = xe.Regridder(ds_xy_grid, ds_out, method=&#39;bilinear&#39;) ds_lonlat_grid = regridder(ds_xy_grid) ds_lonlat_grid = ds_lonlat_grid.rename(lat=&#39;latitude&#39;, lon=&#39;longitude&#39;) # Removing some zero values for plotting correctly ds_lonlat_grid = ds_lonlat_grid.where(ds_lonlat_grid[&#39;Tair_f_tavg&#39;] &gt; 0.01, drop=True) ds_lonlat_grid.attrs = dsi.attrs droped = ds_lonlat_grid.drop_vars([&#39;Rainf_f_inst&#39;, &#39;Tair_f_inst&#39;, &#39;TotalPrecip_inst&#39;]) # Take date time for name of output file time = str(ds_lonlat_grid.time.values) time = time.removesuffix(&#39;T00:00:00.000000000&#39;) # save to netcdf encoding = {&#39;Rainf_f_tavg&#39;:{&#39;zlib&#39;: True, &#39;complevel&#39;: 5, &#39;dtype&#39;: &#39;float32&#39;}, &#39;Tair_f_tavg&#39;:{&#39;zlib&#39;: True, &#39;complevel&#39;: 5, &#39;dtype&#39;: &#39;float32&#39;}, &#39;TotalPrecip_tavg&#39;:{&#39;zlib&#39;: True, &#39;complevel&#39;: 5, &#39;dtype&#39;: &#39;float32&#39;}} droped.to_netcdf((path_output + &quot;regrid_merra_&quot; + time + &quot;.nc&quot;),encoding=encoding) del ds_xy_grid, ds_out, regridder, ds_lonlat_grid, time, encoding, droped . . Comparison between raw and regreded MERRA-2 data are plotted in below. The July 2009 selected for comparison. . ds_raw = xr.open_dataset(&#39;/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/monthly_data/LIS_HIST_200906010000.d01.nc&#39;) ds_reg = xr.open_dataset(&#39;/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/monthly_data/monthly_regrid/regrid_merra_2009-06-01.nc&#39;) # Compare two plots fig, axes = plt.subplots(ncols=2, figsize=(10, 4)) ds_raw.Tair_f_tavg.plot(ax=axes[0]) axes[0].set_title(&quot;Raw data&quot;) ds_reg.Tair_f_tavg.plot(ax=axes[1]) axes[1].set_title(&quot;Regrided data&quot;) fig.tight_layout() . . Now we can select a random point in above plots and extract its air temperature and rain values for more exactly comparison. . # defining a random point for extract climatic variables latitude = 30.48 longitude = 50.58 # raw data abslat = np.abs(ds_raw.lat-latitude) abslon = np.abs(ds_raw.lon-longitude) c = np.maximum(abslon, abslat) ([xloc], [yloc]) = np.where(c == np.min(c)) pnt_raw = ds_raw.sel(north_south=xloc, east_west=yloc) temp_raw = pnt_raw.Tair_f_tavg.values rain_raw = pnt_raw.Rainf_f_tavg.values # regrided data pnt_reg = ds_reg.where((ds_reg.longitude==longitude) &amp; (ds_reg.latitude==latitude), drop=True) temp_reg = pnt_reg.Tair_f_tavg.values rain_reg = pnt_reg.Rainf_f_tavg.values print(&quot;The air temperature in raw Merra-2 is &quot;, format(float(temp_raw), &#39;.2f&#39;),&quot; n&quot; &quot;The air temperature in regrided Merra-2 is &quot;, format(float(temp_reg), &#39;.2f&#39;),&quot; n&quot; &quot;The rain in raw Merra-2 is &quot;, format(float(rain_raw), &#39;.2f&#39;),&quot; n&quot; &quot;The rain in regrided Merra-2 is &quot;, format(float(rain_reg), &#39;.2f&#39;)) . . The air temperature in raw Merra-2 is 303.64 The air temperature in regrided Merra-2 is 303.64 The rain in raw Merra-2 is 0.00 The rain in regrided Merra-2 is 0.00 . Integrated Surface Dataset (Global) . Our data preparation till here was about MERRA-2 dataset. As mentioned earlier, MERRA-2 data type is reanalysis data, since we downscaled this data spatially, verifying this data with real data is necessary. The best database for real hourly climate variables is The Integrated Surface Dataset (ISD). According to its website, ISD is composed of worldwide surface weather observations from over 35,000 stations, though the best spatial coverage is evident in North America, Europe, Australia, and parts of Asia. Parameters included are: air quality, atmospheric pressure, atmospheric temperature/dew point, atmospheric winds, clouds, precipitation, ocean waves, tides and more. ISD refers to the data contained within the digital database as well as the format in which the hourly, synoptic (3-hourly), and daily weather observations are stored. ISD provides hourly data that can be used in a wide range of climatological applications. For some stations, data may go as far back as 1901, though most data show a substantial increase in volume in the 1940s and again in the early 1970s. Currently (2021-08-17), there are over 14,000 &quot;active&quot; stations updated daily in the database. . In our study area boundary, there is 20 stations for surface weather observations. The air temperature and rainfall was downloaded for this stations in target time period. This data need preparation to use in analysis. The vaex library is a useful python package for work with large data with low memory usage. We used this library for preparation of surface weather data. . import vaex dfv = vaex.from_csv(&#39;/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/2654018.csv&#39;) dfv . /home/nilik/MYPROGRAMS/miniconda3/envs/envinfo/lib/python3.9/site-packages/vaex/__init__.py:524: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False. return _from_csv_read(filename_or_buffer=filename_or_buffer, copy_index=copy_index, . # STATION NAME LATITUDE LONGITUDE ELEVATION DATE SOURCE REPORT_TYPE CALL_SIGN QUALITY_CONTROL TMP . 0 | 40875099999 | BANDAR ABBASS INTERNATIONAL, IR | 27.218169 | 56.377875 | 6.7 | 2000-01-01T00:00:00 | 4 | FM-12 | OIKB | V020 | +0142,1 | . 1 | 40875099999 | BANDAR ABBASS INTERNATIONAL, IR | 27.218169 | 56.377875 | 6.7 | 2000-01-01T07:00:00 | 4 | FM-15 | OIKB | V020 | +0260,1 | . 2 | 40875099999 | BANDAR ABBASS INTERNATIONAL, IR | 27.218169 | 56.377875 | 6.7 | 2000-01-01T08:00:00 | 4 | FM-15 | OIKB | V020 | +0280,1 | . 3 | 40875099999 | BANDAR ABBASS INTERNATIONAL, IR | 27.218169 | 56.377875 | 6.7 | 2000-01-01T09:00:00 | 4 | FM-15 | OIKB | V020 | +0290,1 | . 4 | 40875099999 | BANDAR ABBASS INTERNATIONAL, IR | 27.218169 | 56.377875 | 6.7 | 2000-01-01T10:00:00 | 4 | FM-15 | OIKB | V020 | +0300,1 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2,480,209 | 40836199999 | PERSIAN GULF AIRPORT, IR | 27.3666666 | 52.7333333 | 8.0 | 2020-12-31T14:00:00 | 4 | FM-15 | 99999 | V020 | +0220,1 | . 2,480,210 | 40836199999 | PERSIAN GULF AIRPORT, IR | 27.3666666 | 52.7333333 | 8.0 | 2020-12-31T15:00:00 | 4 | FM-15 | 99999 | V020 | +0200,1 | . 2,480,211 | 40836199999 | PERSIAN GULF AIRPORT, IR | 27.3666666 | 52.7333333 | 8.0 | 2020-12-31T16:00:00 | 4 | FM-15 | 99999 | V020 | +0180,1 | . 2,480,212 | 40836199999 | PERSIAN GULF AIRPORT, IR | 27.3666666 | 52.7333333 | 8.0 | 2020-12-31T17:00:00 | 4 | FM-15 | 99999 | V020 | +0160,1 | . 2,480,213 | 40836199999 | PERSIAN GULF AIRPORT, IR | 27.3666666 | 52.7333333 | 8.0 | 2020-12-31T18:00:00 | 4 | FM-15 | 99999 | V020 | +0150,1 | . . The name of surface weather stations are: . dfv.NAME.unique() . . [&#39;BANDAR ABBASS INTERNATIONAL, IR&#39;, &#39;KERMAN, IR&#39;, &#39;RAFSANJAN, IR&#39;, &#39;LAMERD, IR&#39;, &#39;SIRJAN, IR&#39;, &#39;BUSHEHR, IR&#39;, &#39;SHIRAZ SHAHID DASTGHAIB INTERNATIONAL, IR&#39;, &#39;JAM, IR&#39;, &#39;FASA, IR&#39;, &#39;YASOGE, IR&#39;, &#39;ABADEH, IR&#39;, &#39;BANDAR E DAYYER, IR&#39;, &#39;BAFT, IR&#39;, &#39;GACHSARAN, IR&#39;, &#39;LAR, IR&#39;, &#39;FARSI ISLAND, IR&#39;, &#39;YASOUJ, IR&#39;, &#39;KHARG, IR&#39;, &#39;ASALOYEH, IR&#39;, &#39;PERSIAN GULF AIRPORT, IR&#39;] . Some columns in this data should be changed, you can see preparation commands in follow. . Counting values for each station: . station_count = dfv.groupby(by=&#39;NAME&#39;).agg({&#39;count&#39;:&#39;count&#39;}) for x in range(len(station_count)): print (station_count[x]) . [&#39;BUSHEHR, IR&#39;, 267359] [&#39;YASOGE, IR&#39;, 9650] [&#39;JAM, IR&#39;, 54130] [&#39;BANDAR E DAYYER, IR&#39;, 33972] [&#39;FASA, IR&#39;, 190922] [&#39;YASOUJ, IR&#39;, 166293] [&#39;BAFT, IR&#39;, 53720] [&#39;GACHSARAN, IR&#39;, 149147] [&#39;ASALOYEH, IR&#39;, 2] [&#39;SHIRAZ SHAHID DASTGHAIB INTERNATIONAL, IR&#39;, 407263] [&#39;KHARG, IR&#39;, 74317] [&#39;PERSIAN GULF AIRPORT, IR&#39;, 97346] [&#39;LAR, IR&#39;, 152458] [&#39;FARSI ISLAND, IR&#39;, 15] [&#39;BANDAR ABBASS INTERNATIONAL, IR&#39;, 335261] [&#39;ABADEH, IR&#39;, 110148] [&#39;LAMERD, IR&#39;, 32682] [&#39;KERMAN, IR&#39;, 273070] [&#39;RAFSANJAN, IR&#39;, 18811] [&#39;SIRJAN, IR&#39;, 53648] . . Checking temperature column data type: . dfv[&#39;TMP&#39;].data_type . &lt;bound method Expression.data_type of Expression = TMP Length: 2,480,214 dtype: string (column) - 0 +0142,1 1 +0260,1 2 +0280,1 3 +0290,1 4 +0300,1 ... 2480209 +0220,1 2480210 +0200,1 2480211 +0180,1 2480212 +0160,1 2480213 +0150,1 &gt; . . It&#39;s need to be changed the temperature column data type from string to float. So, the &#39;Temp&#39; column values was splitted and then was built new temperature column with correct values. . dfv[&#39;Temp_C&#39;] = dfv[&quot;TMP&quot;].str.split(&quot;,&quot;).apply(lambda x: x[0]) dfv[&#39;Temp_C&#39;].astype(&#39;float32&#39;) . Expression = astype(Temp_C, &#39;float32&#39;) Length: 2,480,214 dtype: float32 (expression) 0 142 1 260 2 280 3 290 4 300 ... 2480209 220 2480210 200 2480211 180 2480212 160 2480213 150 . . Then, we convert temperature values to celsius degree. . dfv[&#39;Temp_C_2&#39;] = (dfv[&#39;Temp_C&#39;].astype(&#39;float32&#39;))/10 . Checking DATA column data type: . dfv.DATE.data_type . &lt;bound method Expression.data_type of Expression = DATE Length: 2,480,214 dtype: string (column) - 0 2000-01-01T00:00:00 1 2000-01-01T07:00:00 2 2000-01-01T08:00:00 3 2000-01-01T09:00:00 4 2000-01-01T10:00:00 ... 2480209 2020-12-31T14:00:00 2480210 2020-12-31T15:00:00 2480211 2020-12-31T16:00:00 2480212 2020-12-31T17:00:00 2480213 2020-12-31T18:00:00 &gt; . . The data type of DATA column should be in datetime64 format, so it was changed. . dfv[&#39;DATE&#39;]=dfv.DATE.astype(&#39;datetime64[ns]&#39;) . . After finalizing preparation of surface data observations, it was saved as a new CSV file. . dfv.export_csv(&quot;/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/improved_stations.csv&quot;) . Comparison of MERRA-2 data with surface data observation data . After regriding MERRA-2 data and cleaning surface observation data, we can compare these data based on station latitude and longitude. For example we want to compare air temperature in Shiraz station for November 2018. . # Import libraries import vaex import numpy as np import xarray as xr import matplotlib.pyplot as plt import xesmf as xe import warnings warnings.filterwarnings(&#39;ignore&#39;) # Opening surface data observations dfv = vaex.from_csv(&#39;/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/improved_stations.csv&#39;) # Opening Merra-2 data ds = xr.open_mfdataset(&quot;/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/monthly_data/*.nc&quot;) ds = ds.set_coords([&#39;lon&#39;,&#39;lat&#39;]) ########## Extracting data from surface weather observation data ########## # Changing &#39;DATE&#39; column data type from string to datetime64 dfv[&#39;DATE&#39;]=dfv.DATE.astype(&#39;datetime64[ns]&#39;) # Filtering dataframe based on &#39;NAME&#39; column station_name = &#39;LAMERD, IR&#39; station = dfv[dfv.NAME == str(station_name)] # Select a certain month data start_date = np.datetime64(&#39;2018-11-01 00:00:00&#39;) end_date = np.datetime64(&#39;2018-11-30 23:00:00&#39;) startDATE = station[(station.DATE &gt; start_date)] rangeDATE = startDATE[(startDATE.DATE &lt; end_date)] # Drop nan values (999.9) edit = rangeDATE[rangeDATE.Temp_C &lt; np.float64(100)] # Calculate monthly mean temperature for 2018-11 trmp_2018_11 = edit.mean(edit[&#39;Temp_C&#39;]) # Print surface monthly air temperature print(&#39;Surface monthly air temperature for [&#39;, station_name, &#39;] is: &#39;, format(float(trmp_2018_11), &#39;.2f&#39;)) ########## Extracting data from rrgrided MERRA-2 data ########## # Sekecting target month ds201811 = ds.sel(time=&#39;2018-11-01&#39;) # Extracting minimum and maximum of lat/lon min_lon = ds201811.lon.min().values min_lat = ds201811.lat.min().values max_lon = ds201811.lon.max().values max_lat = ds201811.lat.max().values # swap dimensions with multi-dimensional coordinates ds_xy_grid = ds201811.rename(north_south=&#39;lat&#39;, east_west=&#39;lon&#39;) ds_out = xr.Dataset({&#39;lat&#39;: ([&#39;lat&#39;], np.linspace(float(min_lat), float(max_lat), int(ds201811.north_south.count()))), &#39;lon&#39;: ([&#39;lon&#39;], np.linspace(float(min_lon), float(max_lon), int(ds201811.east_west.count())))}) regridder = xe.Regridder(ds_xy_grid, ds_out, method=&#39;bilinear&#39;) ds_lonlat_grid = regridder(ds_xy_grid) ds_lonlat_grid = ds_lonlat_grid.rename(lat=&#39;latitude&#39;, lon=&#39;longitude&#39;) # Extracting latitude and longitude values based on the dataframe of target station latitude = station.LATITUDE.unique()[0] longitude = station.LONGITUDE.unique()[0] latitude = round(latitude, 2) longitude = round(longitude, 2) # Extracting the air temperature from merra-2 data pnts = ds_lonlat_grid.where((ds_lonlat_grid.longitude==longitude) &amp; (ds_lonlat_grid.latitude==latitude), drop=True) celsius = (pnts.Tair_f_tavg.values) - 273.15 # Print MERRA-2 monthly air temperature print(&#39;MERRA-2 monthly air temperature for [&#39;, station_name, &#39;] is: &#39;, format(float(celsius), &#39;.2f&#39;)) . . Surface monthly air temperature for [ LAMERD, IR ] is: 23.44 MERRA-2 monthly air temperature for [ LAMERD, IR ] is: 27.52 . In above example we compared real and reanalused air temperature for one location in a specific time, but it&#39;s ideal to compare all locations in all date time period. So, first we will try to extract the real air temperature for all locations in all time and then extract these values from MERRA-2 data. Finally we&#39;ll try to compare all of data in one plot. . Extract surface temperature values for each station in all 252 months . With this code we can plot monthly air temperature of surface weather stations (real data) year by year. . # Import libraries import pandas as pd import numpy as np import hvplot.pandas import warnings warnings.filterwarnings(&#39;ignore&#39;) # Opening surface data observations df_station = pd.read_csv(&#39;/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/improved_stations.csv&#39;) # Changing &#39;DATE&#39; column data type from string to datetime64 df_station[&#39;DATE&#39;]=df_station.DATE.astype(&#39;datetime64[ns]&#39;) # there is one station with two name, improve it! df_station.NAME[df_station.NAME==&#39;YASOUJ, IR&#39;] = &#39;YASOGE, IR&#39; # Drop nan values (999.9) df_station = df_station[df_station.Temp_C &lt; np.float64(100)] # Calculate monthly mean temperature in each year mean_month_staion = df_station.groupby([df_station.DATE.dt.month, df_station.DATE.dt.year, df_station.NAME, round(df_station.LATITUDE, 2), round(df_station.LONGITUDE, 2)])[&quot;Temp_C&quot;].mean() mean_month_staion = mean_month_staion.rename_axis([&#39;MONTH&#39;, &#39;YEAR&#39;, &#39;NAME&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]) #mean_month_staion.hvplot.line(x = &#39;MONTH&#39;, y= &#39;Temp_C&#39;, groupby=[&#39;NAME&#39;, &#39;YEAR&#39;]) . . Extract MERRA-2 temperature values for each station in all 252 months . With these two below codes we can extract monthly air temperature from MERRA-2 dataset based on surface weather stations latitude and longitude and then plot them year by year. . # import required libraries import xarray as xr import pandas as pd import numpy as np import xesmf as xe import warnings warnings.filterwarnings(&#39;ignore&#39;) from tqdm import tqdm # reading MERRA-2 dataset ds = xr.open_mfdataset(&quot;/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/monthly_data/*.nc&quot;) ds = ds.set_coords([&#39;lon&#39;,&#39;lat&#39;]) # Extracting latitude and longitude of surface weather stations df = pd.read_csv(&#39;/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/improved_stations.csv&#39;) df.NAME[df.NAME==&#39;YASOUJ, IR&#39;] = &#39;YASOGE, IR&#39; df_merra = {&#39;NAME&#39;: [], &#39;time&#39;: [], &#39;mean_monthly_temp&#39;:[], &#39;lot&#39;:[], &#39;lon&#39;:[]} # regriding loop for i in tqdm(range(len(ds.time))): dsi = ds.sel(time = ds.time[i]) # Extracting minimum and maximum of lat/lon min_lon = dsi.lon.min().values min_lat = dsi.lat.min().values max_lon = dsi.lon.max().values max_lat = dsi.lat.max().values # swap dimensions with multi-dimensional coordinates ds_xy_grid = dsi.rename(north_south=&#39;lat&#39;, east_west=&#39;lon&#39;) ds_out = xr.Dataset({&#39;lat&#39;: ([&#39;lat&#39;], np.linspace(float(min_lat), float(max_lat), int(dsi.north_south.count()))), &#39;lon&#39;: ([&#39;lon&#39;], np.linspace(float(min_lon), float(max_lon), int(dsi.east_west.count())))}) regridder = xe.Regridder(ds_xy_grid, ds_out, method=&#39;bilinear&#39;) ds_lonlat_grid = regridder(ds_xy_grid) ds_lonlat_grid = ds_lonlat_grid.rename(lat=&#39;latitude&#39;, lon=&#39;longitude&#39;) # Extracting latitude and longitude of surface weather stations for x in range (len(df.NAME.unique())): # Filtering dataframe based on &#39;NAME&#39; column station_name = df.NAME.unique()[x] station = df[df.NAME == str(station_name)] latitude = station.LATITUDE.unique()[0] longitude = station.LONGITUDE.unique()[0] latitude = round(latitude, 2) longitude = round(longitude, 2) # Extracting the air temperature from merra-2 data pnts = ds_lonlat_grid.where((ds_lonlat_grid.longitude==longitude) &amp; (ds_lonlat_grid.latitude==latitude), drop=True) temp_value = pnts.Tair_f_tavg.values if temp_value.size == 0: celsius = -9999.99 else: celsius = float((pnts.Tair_f_tavg.values) - 273.15) #print(station_name, dsi.time.values, str(pnts.Tair_f_tavg.values), latitude, longitude) #data = {&quot;NAME&quot;: station_name, &#39;time&#39;: str(dsi.time.values), # &#39;mean_monthly_temp&#39;:celsius, &#39;lot&#39;:latitude, &#39;lon&#39;:longitude} df_merra[&#39;NAME&#39;].append(station_name) df_merra[&#39;time&#39;].append(str(dsi.time.values)) df_merra[&#39;mean_monthly_temp&#39;].append(celsius) df_merra[&#39;lat&#39;].append(latitude) df_merra[&#39;lon&#39;].append(longitude) del station_name, station, latitude, longitude, pnts, celsius del dsi, min_lon, min_lat, max_lon, max_lat, ds_xy_grid, ds_out, regridder, ds_lonlat_grid # Save output as a CSV file df_merra2 = pd.DataFrame.from_dict(df_merra) df_merra2.to_csv(&#39;./data/monthly_merra2_stations.csv&#39;) . . # Import libraries import pandas as pd import numpy as np import hvplot.pandas import warnings warnings.filterwarnings(&#39;ignore&#39;) # Opening surface data observations df_merra = pd.read_csv(&#39;./data/monthly_merra2_stations.csv&#39;) # Changing &#39;DATE&#39; column data type from string to datetime64 df_merra[&#39;time&#39;]=df_merra.time.astype(&#39;datetime64[ns]&#39;) # Drop nan values (-9999.99) df_merra = df_merra[df_merra.mean_monthly_temp &gt; np.float64(-9999.99)] df_merra.drop([&#39;Unnamed: 0&#39;], axis=1) # Calculate monthly mean temperature in each year mean_month_merra = df_merra.groupby([df_merra.time.dt.month, df_merra.time.dt.year, df_merra.NAME, df_merra.lot, df_merra.lon])[&quot;mean_monthly_temp&quot;].mean() mean_month_merra = mean_month_merra.rename_axis([&#39;MONTH&#39;, &#39;YEAR&#39;, &#39;NAME&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]) #mean_month_merra.hvplot.line(x = &#39;MONTH&#39;, y= &#39;mean_monthly_temp&#39;, groupby=[&#39;NAME&#39;, &#39;YEAR&#39;]) . . Now, we can plot monthly air temperature of MERRA-2 data and surface weather stations simultaneously. . mean_month_all = pd.concat([mean_month_staion, mean_month_merra], axis=1) plt_station = mean_month_all.hvplot.line(x = &#39;MONTH&#39;, y= &#39;Temp_C&#39; , groupby=[&#39;NAME&#39;, &#39;YEAR&#39;], label=&#39;Surface Stations&#39;)#.opts(legend_position=&#39;top_left&#39;) plt_merra = mean_month_all.hvplot.line(x = &#39;MONTH&#39;, y= &#39;mean_monthly_temp&#39; , groupby=[&#39;NAME&#39;, &#39;YEAR&#39;], label=&#39;MERRA-2 Reanalysis&#39;)#.opts(legend_position=&#39;top_left&#39;) compare_plots = (plt_station * plt_merra).opts(legend_position=&#39;top_left&#39;) hvplot.save(compare_plots, &#39;./htmls/compare_plots.html&#39;) . . Plotting Monthly Air Temperature of Real Data VS Reanalysis DATA . It seems that we have a problem with these data! Both patterns of variation in monthly data are similar but with one month lag in each other. Because of this problem, I checked data. In surface weather station we have hourly data that mean monthly data was aggregated from these hourly data. In downscaled merra-2 data we time dimension array of first day of each month such as 2000-01-01T00:00:00.000000000, I assumed that each mean temperature for each item of time in merra-2 data is for its past month, so I changed the time of merra-2 data with below code and then compared the monthly mean air temperature from surface weather stations with new merra-2 data. . # Import libraries import pandas as pd import numpy as np import hvplot.pandas import warnings warnings.filterwarnings(&#39;ignore&#39;) # Opening surface data observations df_merra_new = pd.read_csv(&#39;./data/monthly_merra2_stations.csv&#39;) # Changing &#39;DATE&#39; column data type from string to datetime64 df_merra_new[&#39;time&#39;]=df_merra_new.time.astype(&#39;datetime64[ns]&#39;) # Drop nan values (-9999.99) df_merra_new = df_merra_new[df_merra_new.mean_monthly_temp &gt; np.float64(-9999.99)] df_merra_new.drop([&#39;Unnamed: 0&#39;], axis=1) df_merra_new[&#39;time&#39;] = df_merra_new.time + pd.DateOffset(months=-1) df_merra_new = df_merra_new[df_merra_new[&#39;time&#39;] &gt; &#39;1999-12-01&#39;] # Calculate monthly mean temperature in each year mean_month_merra_new = df_merra_new.groupby([df_merra_new.time.dt.month, df_merra_new.time.dt.year, df_merra_new.NAME, df_merra_new.lot, df_merra_new.lon])[&quot;mean_monthly_temp&quot;].mean() mean_month_merra_new = mean_month_merra_new.rename_axis([&#39;MONTH&#39;, &#39;YEAR&#39;, &#39;NAME&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]) #mean_month_merra.hvplot.line(x = &#39;MONTH&#39;, y= &#39;mean_monthly_temp&#39;, groupby=[&#39;NAME&#39;, &#39;YEAR&#39;]) # compare plots mean_month_all_new = pd.concat([mean_month_staion, mean_month_merra_new], axis=1) ab1 = mean_month_all_new.hvplot.line(x = &#39;MONTH&#39;, y= &#39;Temp_C&#39; , groupby=[&#39;NAME&#39;, &#39;YEAR&#39;], label=&#39;Station&#39;) ab2 = mean_month_all_new.hvplot.line(x = &#39;MONTH&#39;, y= &#39;mean_monthly_temp&#39; , groupby=[&#39;NAME&#39;, &#39;YEAR&#39;], label=&#39;MERRA-2&#39;) compare_plots_new = (ab1 * ab2).opts(legend_position=&#39;top_left&#39;) hvplot.save(compare_plots_new, &#39;./htmls/compare_plots_changed_merra_time.html&#39;) . . In follow you can see the time series of hourly surface weather station data availability identified by station name. . import pandas as pd import hvplot.pandas from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;&quot;)) import warnings warnings.filterwarnings(&#39;ignore&#39;) df = pd.read_csv(&#39;/media/nilik/78F80520F804DDEE/Student_MSc/Javi/Data/Hourly Surface Station Data/improved_stations.csv&#39;) # remove max value of temperature as NAN value df = df[df.Temp_C != 999.9] # there is one station with two name, improve it! df.NAME[df.NAME==&#39;YASOUJ, IR&#39;] = &#39;YASOGE, IR&#39; df[&#39;time_hour&#39;] = pd.to_datetime(df[&#39;DATE&#39;]).dt.hour df[&#39;time_day&#39;] = pd.to_datetime(df[&#39;DATE&#39;]).dt.day df[&#39;time_month&#39;] = pd.to_datetime(df[&#39;DATE&#39;]).dt.month df[&#39;time_year&#39;] = pd.to_datetime(df[&#39;DATE&#39;]).dt.year # Plot time series of data heatplot = df.hvplot.scatter(x=&#39;time_hour&#39;, y=&#39;time_day&#39;,groupby=[&#39;NAME&#39;,&#39;time_year&#39;, &#39;time_month&#39;]) hvplot.save(heatplot, &#39;./htmls/scatterplot.html&#39;) . .",
            "url": "nilick.github.io/hmbnn-blog/merra2/downscale/ldt/drought/2021/08/26/Anaysis_Process_Merra2.html",
            "relUrl": "/merra2/downscale/ldt/drought/2021/08/26/Anaysis_Process_Merra2.html",
            "date": " • Aug 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How to create own your blog? (My Notes)",
            "content": "Building a blog with Fastpages . For create and check blog posts in localhost, it needs dicker-compose. For install it in Ubuntu first in base environment (src=https://docs.docker.com/compose/install/): | sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose . Next for permissions: sudo chmod +x /usr/local/bin/docker-compose . | Now create a repository in github for build fastpages blog: | https://github.com/fastai/fastpages/generate | . rep name = hmblog | create repository from template | in new rep created after a few moments, click on Pull requests and then initial Setup | create an ssh key-pair from:https://8gwifi.org/sshfunctions.jsp | with RSA and 4096 options Generate-SSH-Keys | copy all text in Private Key and pate it in Action Secret of own rep with name of SSH_DEPLOY_KEY | copy all text in Public Key and paste it in Deploy keys and check Allow write access with name of blog-key. | go back to Initial Setup page and in pagedown click on the Merge pull request and then Confirm request. | after a few moments in code page click on url=&quot;https://xxx.github.io/hbnn/&quot; and see the preliminary blog. | _config.yml is for change some metadata such as title, date, category,... | for edit this file open it and change desired texts then commit changes. | . BECAUSE THERE ARE SOME PROBLEMS IN WORK WITH DOCKER IN LOCALHOST IT IS BETTER DONE ANY CHANGES IN GITHUB WEBSITE! . because of fast blogging it is best choice to run in localhost and then upload completed blog posts. | for do it first install &quot;docker engine&quot; and &quot;docker-compose&quot; | for install docker: . # DOCKER ENGINE sudo apt-get remove docker docker-engine docker.io containerd runc sudo apt-get update sudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu (lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null # FOR VERIFY INSTALLED DOCKER ENGINE sudo docker run hello-world # DOCKER-COMPOSE sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose . | . for do it, we need docker and first created repository must be downloaded in local drive. | after download and unzip the repository, open terminal in root of local repository. | sudo make build # (this is for fist time only) | sudo make server | open blog created locally with rep name: http://0.0.0.0:4000/hbnn/ | for stop server &quot;ctrl + c&quot; | EDIT &quot;_config.yml&quot; for show an image side of post title: . show_image: true . | Save &quot;_config.yml&quot; and sudo make server . | (for more reading: https://docs.docker.com/config/pruning/ &amp; https://www.digitalocean.com/community/tutorials/how-to-remove-docker-images-containers-and-volumes) . for cleanup all images, containers and volumes locally use: list of all images sudo docker images -a list of container sudo docker container ls list of volumes sudo docker volume ls . Delete anything docker system prune docker system prune --volumes docker image prune -a . | . for add new jupyter notebook as a new post in blog just add your notebook.ipynb file in _notebook folder. | for deploy various posts with different plots I created some notebooks for show them. | for remove &quot;Subscribe&quot; in footer page based on: https://github.com/jekyll/minima/issues/553#issuecomment-725082582, first add a file name footer.html in _include folder with up link code then run server again to emit rss feed in footer. | Github Desktop Step by Step . Download last relased Github Desktop deb format from: https://github.com/shiftkey/desktop/releases | Open terminal and run update commandas: sudo apt update &amp;&amp; sudo apt install -f | Install Github Desktop package: sudo dpkg -i fileName.deb | After install open package in terminal with: Github desktop | Getting started with Github Desktop: Open GitHub Desktop. Click ‘Sign in to GitHub.com’ | Sign in using your GitHub username &amp; password | Update your user information if necessary | Click ‘Finish’ | Create new repository OR Clone available repository in github.com to local github desktop directory. | With add any file(s) or edit any availe files in github repository, it will be show in github desktop in left sidebar changes tab. | To push the changes in github, you must first type a comment as description in summary (required) in left sidebar and then commit to master. | After commited changes, now it is time to Push origin. | Now you can check your github&#39;s repository and can see changes. |",
            "url": "nilick.github.io/hmbnn-blog/blog/fastpages/github/2021/08/25/_create_blog.html",
            "relUrl": "/blog/fastpages/github/2021/08/25/_create_blog.html",
            "date": " • Aug 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "How to write a scientific paper?",
            "content": "Academic Writing from Paragraph to Essay . by Doeothy E Zematch &amp; Lisa A Rumisek . This book is a combination text and workbook. . The course combines a process approach to writing with a pragmatic approach to teaching the basics of writing. . . The introduction presents process writing. . First work is recognising and identifying key writing structure from modal paragraphs and essays. . The manipulation of structures in short, manageable tasks should be done next. . Finally, it could be applied to own writing. . Critical thinking is emphasized, because it causes aware of the impact of choice of words, sentences, and organisational techniques on the effectiveness of writing. . The focus throughout on academic writing. . Unit 1-6: Analysing and writing the types of paragraphs that commonly occur on academic contexts. . Unit 7: Writing two-paragraph texts, in preparation for longer assignments. . Unit 8-11: Applying what learned about paragraphs to essay writing. . unit 12: Discussing strategies for timed essay writing, including understanding instructions, time-management techniques, and methods for organising information. . Academic writing in English may be different not only from academic writing in your own language, but even from other writing in English. . Me will learn how important the reader is to the writer, and how to express clearly and directly what I meant to communicate. . Introduction: Process Writing . Words for understanding the writer process: . Step: one thing in a series of things you do . Topic: subject; what the piece of writing is about . Gather: to find and collect together . Organise: to arrange in a clear, logical way . Paragraph: a group of related sentences . Essay: a short piece of writing, at least three paragraphs long . Proofread: to check a piece of writing for errors . Edit: to change or correct a piece or writing . . The six steps of the writing process . When we write we do more than just put words together to make sentences . . 1- Pre-writing has three steps: . . 2- Drafting has one step includes: . Step four (Write): Write your paragraph or essay from start to finish. . Use your notes about your ideas and organisation. . 3- Reviewing and revising has one step includes: . Step five(Review structure and content): Check what you have written. . Read your writing silently to yourself or aloud. . Look for places where you can add more information, and check to see if you have any unnecessary information. . Getting a reader&#39;s opinion is a good way to know if your writing is clear and effective. . 4- Rewriting has one step includes: . Step six(Revise structure and content): Use your ideas from step five to rewrite your text, making improvements to structure and content. . You might need to explain something more clearly, or add more details. . You may even need to change your organisation so that your text is more logical. . Together, step 5 and 6 can be called editing. . Proofread: Read your text again and check your spelling and grammar and think about the words you have chosen to use. . Make final correction: Check that you have corrected the errors you discovered in steps 5 and 6 and make other changes you want to make. . Pre-Writing: Getting Ready to Write . How to: . - choose and narrow a topic - gather ideas - edit ideas . What is pre-writing? . Before you begin writing, you decide what you are going to write about. . Then you plan what you are going to write. . This process is called pre-writing. . How to choose a topic for a paragraph? . A paragraph is a group of five to ten sentences that give information about a topic. . Choose a topic that isn&#39;t: . - too narrow (limited, brief). - too broad (general). . A narrow topic will not have enough ideas to write about.(For example ages of my parents) . A broad topic will have too many ideas for just one paragraph. Most paragraphs are five to ten sentences long. (For example schools) . What is brainstorming? . Brainstorming is _a way of gathering ideas about a topic. . In brainstorming thousands of ideas &#39;raining&#39; down onto your paper! . When you brainstorm, write down every idea that comes to you. . Don&#39;t worry now about whether the ideas are good or silly, useful or not. . You can decide that later. Right now, you are gathering as many ideas as you can. .",
            "url": "nilick.github.io/hmbnn-blog/writing/paper/2021/07/06/Paper-Writing.html",
            "relUrl": "/writing/paper/2021/07/06/Paper-Writing.html",
            "date": " • Jul 6, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "GIS & RS learning and notes",
            "content": "GIS . 25 Map Types: Brilliant Ideas to Build Unbeatable Maps https://gisgeography.com/map-types/ | . Python . A Python package for geospatial analysis and interactive mapping in a Jupyter environment https://leafmap.org/ | . Awesome-EarthObservation-Code https://github.com/acgeospatial/awesome-earthobservation-code &amp; http://www.acgeospatial.co.uk/awesome-earthobservation-code/ | . Downloading YouTube Videos https://github.com/pytube/pytube | . Using Pandas and Python to Explore Your Dataset https://realpython.com/pandas-python-explore-dataset/ | . Create Beautiful Tkinter GUIs by Drag and Drop https://github.com/ParthJadhav/Tkinter-Designer | . A simple way of creating movies from xarray objects https://github.com/jbusecke/xmovie | . RS . Observing Earth From Space https://www.futurelearn.com/courses/observing-earth-from-space | . ORNL DAAC https://daac.ornl.gov/resources/learning/ | . Educational resource for exploring satellite images https://www.edu-sat.com/?lang=en | . E-Learning LP DAAC https://lpdaac.usgs.gov/resources/e-learning/ | . Methane Emissions from Dairy Sources https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1882 | .",
            "url": "nilick.github.io/hmbnn-blog/gis/python/rs/learning/2021/07/03/Learning-GIS-RS.html",
            "relUrl": "/gis/python/rs/learning/2021/07/03/Learning-GIS-RS.html",
            "date": " • Jul 3, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Resources for ML",
            "content": "Machine Learning Algorithms with Python, All Machine Learning Algorithms Explained with Python https://thecleverprogrammer.com/2020/11/27/machine-learning-algorithms-with-python/ | Free courses from Universities https://collegecompendium.goldin.io/search?q=machine-learning | Dust in the Machine https://earthdata.nasa.gov/learn/articles/dust-ml | Data Preparation for Machine Learning (7-Day Mini-Course) https://machinelearningmastery.com/data-preparation-for-machine-learning-7-day-mini-course/ | Linear Algebra for Machine Learning (7-Day Mini-Course) https://machinelearningmastery.com/linear-algebra-machine-learning-7-day-mini-course/ | How to Calculate Correlation Between Variables in Python https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/ | Need Help Getting Started with Applied Machine Learning? https://machinelearningmastery.com/start-here/ | ML YouTube Courses https://github.com/dair-ai/ML-YouTube-Courses | Statistics and probability https://www.khanacademy.org/math/statistics-probability | Machine Learning for Beginners https://github.com/microsoft/ML-For-Beginners | 11- Data Science: Machine Learning https://www.edx.org/course/data-science-machine-learning . 12- My Advice To Machine Learning Newbies After 3 Years In The Game https://towardsdatascience.com/my-advice-to-machine-learning-newbies-after-3-years-in-the-game-6eef381f540 . 13- So You Want to Do Machine Learning But Don’t Know Where to Start https://towardsdatascience.com/so-you-want-to-do-machine-learning-but-dont-know-where-to-start-3fba1529bbcd .",
            "url": "nilick.github.io/hmbnn-blog/ml/python/machine%20learning/2021/07/02/ML-Resources.html",
            "relUrl": "/ml/python/machine%20learning/2021/07/02/ML-Resources.html",
            "date": " • Jul 2, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Julia Programming",
            "content": "Install Julia on Jupyterlab . sudo apt update sudo apt install wget -y . Download latest version of Julia from: https://julialang.org/downloads/ [Generic Linux on x86 - 64bit] . tar -xvzf julia-1.6.1-linux-x86_64.tar.gz sudo mv julia-1.6.1 /opt/julia . Add /opt/julia/bin directory to your PATH. . sudo gedit ~/.bashrc export PATH=$PATH:/opt/julia/bin . Source the bashrc file to update the settings. . source ~/.bashrc . Validate your current PATH settings. . echo $PATH . Confirm julia binary file is executable from your shell terminal session. . julia --version . Start Julia shell by running the following command: . julia . Add Julia to Jupyter Notebook . using Pkg Pkg.add(&quot;IJulia&quot;) . Launch Jupyterlab and open Julia . Courses . -Free Course https://juliaacademy.com/p/julia-programming-for-nervous-beginners [My order ID: 87188600] [vpn] . -Tutorials https://julialang.org/learning/tutorials/ . -From zero to Julia! https://techytok.com/from-zero-to-julia/ .",
            "url": "nilick.github.io/hmbnn-blog/julia/programming/2021/07/02/Julia-Programming.html",
            "relUrl": "/julia/programming/2021/07/02/Julia-Programming.html",
            "date": " • Jul 2, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Ideas for research",
            "content": "1- Carbon dioxide emission plumes from a large power station detected from space https://eo4society.esa.int/2021/06/14/carbon-dioxide-emission-plumes-from-a-large-power-station-detected-from-space/ .",
            "url": "nilick.github.io/hmbnn-blog/idea/research/2021/07/02/Ideas.html",
            "relUrl": "/idea/research/2021/07/02/Ideas.html",
            "date": " • Jul 2, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Machine Learning with Python",
            "content": "(Ref: https://machinelearningmastery.com) . Machine learning is broken down into a 5-step process: . Step 1: Adjust Mindset. Believe you can practice and apply machine learning. What is Holding you Back From Your Machine Learning Goals? Why Machine Learning Does Not Have to Be So Hard How to Think About Machine Learning Find Your Machine Learning Tribe Step 2: Pick a Process. Use a systemic process to work through problems. Applied Machine Learning Process Step 3: Pick a Tool. Select a tool for your level and map it onto your process. Beginners: Weka Workbench. Intermediate: Python Ecosystem. Advanced: R Platform. Best Programming Language for Machine Learning Step 4: Practice on Datasets. Select datasets to work on and practice the process. Practice Machine Learning with Small In-Memory Datasets Tour of Real-World Machine Learning Problems Work on Machine Learning Problems That Matter To You Step 5: Build a Portfolio. Gather results and demonstrate your skills. Build a Machine Learning Portfolio Get Paid To Apply Machine Learning Machine Learning For Money . What Is Holding You Back From Your Machine Learning Goals? . The first question I ask them is what is stopping them from getting started? . In this post, I want to touch on some self-limiting beliefs I see crop up in my email exchanges and discussions with coaching students. . Self-Limiting Belief . A self-limiting belief is something that you assume to be true that is limiting your progress. You presuppose something about yourself or about the thing you want to achieve. The problem is you hold that belief to be true and you don’t question it. . 3 types of self-limiting . If-then Beliefs: e.g. If I get started in machine learning, I will fail because I am not good enough. | Universal Beliefs: e.g. All Data Scientists have a Ph.D. and are mathematics rock gods. | Personal and Self-Esteem Beliefs: e.g. I’m not good enough to be a machine learner. | . Waiting To Get Started . I think the biggest class of limiting belief I see is the belief that you cannot get started until you have some specific prior knowledge. . The problem is that the prior knowledge you think you need is either not required or is so vast in scope that even experts in that subject don’t know it all. . For example: “I need to KNOW statistics“. See how ambiguous that belief is. How much statistics, what areas of statistics and why do you need to know them before you can start your investigation into machine learning? . I can’t get into machine learning until… . …I get a degree or higher degree …I complete a course …I am good at linear algebra …I know statistics and probability theory …I have mastered the R programming language . You can get started in machine learning today, right now. Run your first classifier in 5 minutes. You’re in. Now, start blocking out what it is from machine learning that you really want? . Awaiting Perfect Conditions . Another class of self-limiting belief is where you are waiting for the perfect environment or conditions before taking the leap. Things will never be perfect, leap and make a mess, then leap again. . I can’t get started in machine learning because… . …I don’t have the time right now …I don’t have a fast CPU, GPU or a bazillion MB of RAM …I am just a student right now …I am not a good programmer at the moment …I am very busy at work right now . It does take a lot of time and effort to get good at machine learning, but not all at once and not all at the beginning. . You can make good progress with a few hours a week, or tens of minutes per day. There are plenty of small snack-sized tasks you could take on to get started in machine learning. . You can get started, it is just going to take some sacrifice, like all good things in life. . Struggling or Tried and Failed . Machine learning is hard but no harder than other technical skills like programming. It takes persistence and dedication. It’s applied and empirical and demands trial and error. . I can’t get into machine learning because… . …I feel overwhelmed …I don’t understand x …I will never be as good as y …I don’t know what to do next …I can’t get my program to work . My advice is to cut scope or change direction. . What is your self-limiting belief? . Do you have a self-limiting belief? Think about it. What are your goals and why do you think you are not there yet? . Do you have a goal to get into machine learning, to become a data scientist or a machine learning engineer but have not taken the first step? . Are you waiting to acquire some perfect set of skills before getting started? Are you waiting for the perfect conditions before getting started? Have you taken a first step and abandoned the trail? Where do you want to be and what are you struggling with? . What If I Am Not Good At Mathematics . Problen: The think that mathematicians are smarter than they are and that they cannot excel in a subject until they “know the math”. . I have seen this first hand, and I have seen it stop people from getting started. . In this post, I want to convince you that you can get started and make great progress in machine learning without being strong in mathematics. . Get Started and Learn by Doing . I didn’t learn boolean logic before I started programming. . I followed an empirical path that involved trial and error. It is slow and I wrote a lot of bad code, but I was passionately interested and I could see progress. . I hunted for conceptual and practical tools I could use to overcome the limitations I was actually experiencing. This was a powerful learning tool. . The Danger Zone . I like it when my programs don’t work. It means I have to roll up my sleeves and really understand what is going on. . You can get a long way by copy and pasting code without really understanding it. You only need to understand blocks of code as functional units that do a thing you need done. Glue enough of them together and you have a program that solves the problem you need solved. . This empirical hackery is a great way to learn fast, but a terrifying way to build production systems. . This is an important distinction to make. The often spoken of “danger zone” is when systems built from empirical learning are made operational and the author does not really know how it works or what the results actually mean. . The Technician . You can get started in machine learning today, empirically. Three options available to you are: . Learn to drive a tool like scikit-learn. | Use libraries that provide algorithms and write little programs | Implement algorithms yourself from tutorials and books. | This can be the path of the technician from beginner to intermediate that is learning the mathematics required for a technique, just-in-time. . Define small problems, solve them methodically and present the results of what you have learned on your blog. . There will be interesting algorithms that you will want to know more about, such as what a particular parameter actually does when you change it or how to get better results from a particular algorithm. . This will drive you to want (need) to understand how that technique really works and what it is doing. . You can remain the empiricist. I call this the path of the technician. . You can build up an empirical intuition of which methods to use and how to use them. You can also learn just enough algebra to be able to read algorithm descriptions and turn them into code. . There is a path here for the skilled technician to create tools, plug-in’s and even operational systems that use machine learning. . The technician is contrasted to the theoretician at the other end of the scale. The theoretician can: . 1. Internalize existing methods. 2. Propose extensions to existing methods. 3. Devise entirely new methods. . The theoretician may be able to demonstrate the capability of a method in the abstract, but is likely insufficiently skilled to turn the methods into code beyond prototype demonstration systems at best. . You can learn as little or as much mathematics as you like, just in time. Focus on your strengths and be honest about your limitations. . Mathematics is Critical, Later . If you have to learn linear algebra just-in-time, why not learn it fully more completely up front and understand the machine learning methods at this deep level from the beginning? . This is certainly an option, perhaps the most efficient option which is why it is the path used to teach in university. It’s just not the only option available to you. . Just like learning to program by starting with logic and abstract concepts, internalizing machine learning theory may not be the most efficient way for you to get started. . You learned that the technician can learn the mathematical representations and descriptions of machine learning algorithms just-in-time. You also learned that the danger zone for the technician is overconfidence and the risk of putting systems into production that are poorly understood. . Why Machine Learning Does Not Have to Be So Hard? . Useful skills we use every day like reading, driving, and programming were not learned this way and were in fact learned using an inverted top-down approach. . This top-down approach can be used to learn technical subjects directly such as machine learning, which can make you a lot more productive a lot sooner, and be a lot of fun. . In contrast, technical topics like mathematics, physics, and even computer science are taught using a bottom-up approach. . You will know: . - The bottom-up approach used in universities to teach technical subjects and the problems with it. - How people learn to read, drive, and program in a top-down manner and how the top-down approach works. - The frame of machine learning and even mathematics using the top-down approach to learning and how to start to make rapid progress as a practitioner. . This is an important blog post, because I think it can really help to shake you out of the bottom-up, university-style way of learning machine learning. . This post is divided into seven parts; they are: . 1. Bottom-Up Learning 2. Learning to Read 3. Learning to Drive 4. Learning to Code 5. Top-Down Learning 6. Learn Machine Learning 7. Learning Mathematics . Bottom-Up Learning . Think back to high-school or undergraduate studies and the fundamental fields you may have worked through: examples such as: Mathematics, as mentioned, Biology, Chemistry etc. . Think about how the material was laid out, week-by-week, semester-by-semester, year-by-year. Bottom-up, logical progression. . The problem is, the logical progression through the material may not be the best way to learn the material in order to be productive. . We are not robots executing a learning program. We are emotional humans that need motivation, interest, attention, encouragement, and results. . If you have completed a technical subject, think back to how to you actually learned it. I bet it was not bottom-up. . Learning to Drive . I remember hiring a driving instructor and doing driving lessons. Every single lesson was practical, in the car, practicing the skill I was required to master, driving the vehicle in traffic. . Here’s what I did not study or discuss with my driving instructor: . The history of the automobile. The theory of combustion engines. The common mechanical faults in cars. The electrical system of the car. The theory of traffic flows. . In fact, I never expect to learn these topics. I have zero need or interest and they will not help me realize the thing I want and need, which is safe and easy personal mobility. . If the car breaks, I’ll call an expert. . Learning to Code . I started programming without any idea of what coding or software engineering meant. . Top-Down Learning . The bottom-up approach is not just a common way for teaching technical topics; it looks like the only way. . The designers of university courses, masters of their subject area, are trying to help. They are laying everything out to give you the logical progression through the material that they think will get you to the skills and capabilities that you require (hopefully). . And as I mentioned, it can work for some people. . It does not work for me, and I expect it does not work for you. . Don’t start with definitions and theory. Instead, start by connecting the subject with the results you want and show how to get results immediately. . Lay out a program that focuses on practicing this process of getting results, going deeper into some areas as needed, but always in the context of the result they require. . Be careful not to use traditional ways of thinking or comparison if you take this path. . It is iterative: Topics are revisited many times with deeper understanding. It is imperfect: Results may be poor in the beginning, but improve with practice. It requires discovery: The learner must be open to continual learning and discoverery. It requires ownership: The learner is responsible for improvement. It requires curiosity: The learner must pay attention to what interests them and follow it. . Learning Machine Learning . Are you following a top-down type approach but are riddled with guilt, math envy, and insecurities? . You are not alone; I see this every single day in helping beginners on this website. . To connect the dots for you, I strongly encourage you to study machine learning using the top-down approach. . Don’t start with precursor math. Don’t start with machine learning theory. Don’t code every algorithm from scratch. . 1- Start by learning how to work through very simple predictive modeling problems using a fixed framework with free and easy-to-use open source tools. . 2- Practice on many small projects and slowly increase their complexity. . 3- Show your work by building a public portfolio. . You can learn machine learning by practicing predictive modeling, not by studying math and theory. . Not only is this the way I learned and continue to practice machine learning, but it has helped tens of thousands of my students (and the many millions of readers of this blog). . A top-down approach might be to: . - Implement the method in a high-level library such as scikit-learn and get a result. - Implement the method in a lower-level library such as NumPy/SciPy and reproduce the result. - Implement the method directly using matrices and matrix operations in NumPy or Octave. - Study and explore the matrix arithmetic operations involved. - Study and explore the matrix decomposition operations involved. - Study methods for approximating the eigendecomposition of a matrix. And so on… . The goal provides the context and you can let your curiosity define the depth of study. . Painted this way, studying math is no different to studying any other topic in programming, machine learning, or other technical subjects. . - The bottom-up approach used in universities to teach technical subjects and the problems with it. - How people learn to read, drive, and program in a top-down manner and how the top-down approach works. - The frame of machine learning and even mathematics using the top-down approach to learning and how to start to make rapid progress as a practitioner. . How to Think About Machine Learning . You can achieve impressive results with machine learning and find solutions to very challenging problems. . But this is only a small corner of the broader field of machine learning often called predictive modeling or predictive analytics. . In this post, you will discover how to change the way you think about machine learning in order to best serve you as a machine learning practitioner. . - What machine learning is and how it relates to artificial intelligence and statistics. - The corner of machine learning that you should focus on. - How to think about your problem and the machine learning solution to your problem. . Machine learning is a large field of study, and not all much of it is going to be relevant to you if you’re focused on solving a problem. . What is Machine Learning? . Machine learning is a field of computer science concerned with programs that learn. . There are many types of learning, many types of feedback to learn from, and many things that can be learned. . This could encompass diverse types of learning, such as: . - Developing code to investigate how populations of organisms “learn” to adapt to their environment over evolutionary time. - Developing code to investigate how one neuron in the brain “learns” in response to stimulus from other neurons. - Developing code to investigate how ants “learn” the optimal path from their home to their food source. . Another case that you may be more familiar with is: . - Developing code to investigate how to “learn” patterns in historical data. . This is less glamorous, but is the basis of the small corner of machine learning in which we as practitioners are deeply interested. . What About Statistics? . Statistics, or applied statistics with computers, is a sub-field of mathematics that is concerned with describing and understanding the relationships in data. . This could encompass diverse types of learning such as: . - Developing models to summarize the distribution of a variable. - Developing models to best characterize the relationship between two variables. - Developing models to test the similarity between two populations of observations. . It also overlaps with the corner of machine learning interested in learning patterns in data. . Many methods used for understanding data in statistics can be used in machine learning to learn patterns in data. These tasks could be called machine learning or applied statistics. . Your Machine Learning . Machine learning is a large field of study, and it can help you solve specific problems. . But you don’t need to know about all of it. . In fact, when it comes to learning relationships in data: . - You’re not investigating the capabilities of an algorithm. - You’re not developing an entirely new theory or algorithm. - You’re not extending an existing machine learning algorithm to new cases. . So what parts of machine learning do you need to focus on? . I think there are two ways to think about machine learning: . - In terms of the problem you are trying to solve. - In terms of the solution you require. . Your Machine Learning Problem . Your problem can best be described as the following: . Find a model or procedure that makes best use of historical data comprised of inputs and outputs in order to skillfully predict outputs given new and unseen inputs in the future. . Based on this description: . It discards entire sub-fields of machine learning, such as unsupervised learning, to focus on one type of learning called supervised learning . Your Machine Learning Solution . The solution you require is best described as the following: . A model or procedure that automatically creates the most likely approximation of the unknown underlying relationship between inputs and associated outputs in historical data. . In fact, problems of this type resist top-down hand-coded solutions. If you could sit down and write some if-statements to solve your problem, you would not need a machine learning solution. It would be a programming problem. . The type of machine learning methods that you need will learn the relationship between the inputs and outputs in your historical data. . Applied Machine Learning Process . Over time, working on applied machine learning problems you develop a pattern or process for quickly getting to good robust results. . Once developed, you can use this process again and again on project after project. The more robust and developed your process, the faster you can get to reliable results. . The skeleton of the process for working a machine learning problem . This can be as a starting point or template on your next project . 5-step process . 1. Define the Problem 2. Prepare Data 3. Spot Check Algorithms 4. Improve Results 5. Present Results . There is a lot of flexibility in this process. . For example, the “prepare data” step is typically broken down into analyze data (summarize and graph) and prepare data (prepare samples for experiments). . It’s a great big production line that I try to move through in a linear manner. . The great thing in using automated tools is that you can go back a few steps (say from “Improve Results” back to “Prepare Data”) and insert a new transform of the dataset and re-run experiments in the intervening steps to see what interesting results come out and how they compare to the experiments you executed before. . NOTE: The process I use has been adapted from the standard data mining process of knowledge discovery in databases (or KDD). .",
            "url": "nilick.github.io/hmbnn-blog/machine%20learning/python/course/2021/06/08/ML-comments.html",
            "relUrl": "/machine%20learning/python/course/2021/06/08/ML-comments.html",
            "date": " • Jun 8, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "LDT Installing in ubuntu",
            "content": "In this post, I want to describe how to install LDT in ubuntu step-by-step. . Necessary Packages . 1- First conda deactivate . 2- check installed packages: . apt list --installed . 3- Install CMAKE . sudo apt install cmake . 4- Installing GCC &amp; Fortran on Ubuntu . sudo apt update . sudo apt install build-essential . sudo apt-get install manpages-dev . sudo apt install gcc gfortran . gcc --version . gfortran --version . 5- Check Perl version: . perl -v . for install perl:&gt; sudo apt-get install perl . 6- Install lapack: . sudo apt-get install liblapack-dev . 7- How to check if the MPI already installed on my machine? . ompi_info . install:&gt; sudo apt install openmpi-bin . 8- Install NetCDF and NetCDF-Fortran (src:https://cloud-gc.readthedocs.io/en/stable/chapter04_developer-guide/install-basic.html) . sudo apt-get install libnetcdf-dev libnetcdff-dev . Check NetCDF-C configuration:&gt; nc-config --all Check NetCDF-Fortran configuration: . nf-config --all . 9- Install OpenJPEG: . sudo apt install libopenjp2-7 . sudo apt-get install libopenjp2-7-dev . 10- Install ecCodes: . download the latest ecCodes from: . https://confluence.ecmwf.int/display/ECC/Releases . (src: https://confluence.ecmwf.int/display/ECC/ecCodes+installation) . tar -xzf eccodes-x.y.z-Source.tar.gz . mkdir build ; cd build . cmake -DCMAKE_INSTALL+PREFIX=/home/nilik/MYPROGRAMS/LDT/ecCodes . ../eccodes-2.22.0-Source . ... . make . ctest . sudo make install . then:&gt; sudo apt install python3-pip . pip install eccodes . 11- Check for installed HDF4, HDF5, GDAL and GeoTIFF: . dpkg -l | grep hdf4 . dpkg -l | grep hdf5 . dpkg -l | grep gdal . dpkg -l | grep geotiff . If each of them is not installed you can use the method from:https://github.com/NASA-LIS/LISF/blob/master/docs/LDT_users_guide/build.adoc . 12- For use Grib API it is necessary to instal jasper: . sudo apt install jasper . 13- sudo apt-get install libpng-dev . 14- sudo apt-get install zlib1g-dev . 15- sudo apt-get install libjpeg8-dev . 16- sudo apt-get install flex bison . 17- Install SZIP: . Download src from https://support.hdfgroup.org/ftp/lib-external/szip/2.1.1/src/szip-2.1.1.tar.gz Unzip and go in it within terminal: ./configure –-prefix=/where_to_install (such as:/home/nilik/MYPROGRAMS/szip) make . make check . make install . 18- Install HDF4 . Download HDF4.2.15 from https://portal.hdfgroup.org/display/support/HDF+4.2.15#files . Extract the source from the hdf-X.Y.Z.tar file and change directory to hdf-4.2.15 . [ For prevent error:configure: error: couldn&#39;t find rpc headers: Replace HDF4 version from 4.2.13 to 4.2.15 to fix the error. Then you will get the next configure error: . configure: error: couldn&#39;t find rpc headers . To fix it edit &quot;configure&quot; file from HDF4 v4.3.15 sources and replace line 23672: . CPPFLAGS=&quot;$SYSCPPFLAGS -I/usr/include/tirpc&quot; . to . CPPFLAGS=&quot;$SYSCPPFLAGS -I/usr/include/tirpc&quot; unset ac_cv_header_rpc_rpc_h . SRC: https://gitlab.orfeo-toolbox.org/maja/maja/-/issues/207 ] . For Install HDF4: | . It is very very important first: . sudo -i . Then:&gt; cd /home/nilik/Temp/hdf-4.2.15 (where unrared HDF4 source code) Then: . export FCFLAGS=&quot;-w -fallow-argument-mismatch -O2&quot; . export FFLAGS=&quot;-w -fallow-argument-mismatch -O2&quot; . export FFLAGS=&quot;-w -fno-second-underscore -O2&quot; . ./configure --with-zlib=/usr --with-jpeg=/usr --disable-netcdf --prefix=/home/nilik/MYPROGRAMS/hdf4 . gmake . gmake check . gmake install . After install it is important to check that is there “hdf.f90” in “/home/nilik/MYPROGRAMS/hdf4/include” directory or not? It should be created. . (https://www.programmersought.com/article/34565557036/) . 19- Install HDFEOS: . download HDFEOS from (with VPN): https://wiki.earthdata.nasa.gov/display/DAS/Toolkit+Downloads . https://git.earthdata.nasa.gov/rest/git-lfs/storage/DAS/hdfeos/cb0f900d2732ab01e51284d6c9e90d0e852d61bba9bce3b43af0430ab5414903?response-content-disposition=attachment%3B%20filename%3D%22HDF-EOS2.20v1.00.tar.Z%22%3B%20filename*%3Dutf-8%27%27HDF-EOS2.20v1.00.tar.Z . ./configure --prefix=/usr/local/hdfeos CC=&#39;/usr/lib/bin/h4cc -Df2cFortran&#39; --enable-install_include . sudo make . sudo make install . Install ESMF final method . src:https://earthscience.stackexchange.com/questions/18758/how-to-install-esmf-and-esmfpy-in-ubuntu-using-gfortran-gcc-python . Very important NOTE: . For everytime to install new or refresh ESMF installation it is necessary to check below codes and then define the following environmental variables! . sudo apt-get install git tcsh pkg-config sudo apt-get install gfortran sudo apt-get install netcdf-bin libnetcdf-dev libnetcdff-dev sudo apt-get install openmpi-bin libopenmpi-dev sudo apt-get install libnetcdff-dev . 1- Download ESMF == http://earthsystemmodeling.org/download/ . 2- Extract ESMF downloaded == /home/nilik/Temp/esmf-ESMF_8_1_1 . 3- login as root using . sudo -i . 4- Define the following environment variables: . cd /home/nilik/Temp/esmf-ESMF_8_1_1 . export ESMF_DIR=/home/nilik/Temp/esmf-ESMF_8_1_1 . export ESMF_INSTALL_PREFIX=/home/nilik/MYPROGRAMS/LDT/ESMF . export ESMF_OS=Linux . export ESMF_NETCDF=/usr/include . export ESMF_COMM=mpiuni . export ESMF_NETCDF_INCLUDE=/usr/local/include . export ESMF_NETCDF_LIBS=&quot;-lnetcdf -lnetcdff&quot; . export ESMF_NETCDF_LIBPATH=/usr/local/lib . 5- Run the following syntax to make the library ESMF:&gt; make all . make install . make installcheck . export ESMF_INC=$ESMF_INSTALL_PREFIX/include . export ESMF_LIB=$ESMF_INSTALL_PREFIX/lib/libO/Linux.intel.64.intelmpi.default . export ESMFMKFILE=$ESMF_INSTALL_PREFIX/lib/libO/Linux.intel.64.intelmpi.default/esmf.mk . 6- Install the ESMF python library: . cd /home/nilik/Temp/esmf-ESMF_8_1_1/src/addon/ESMPy . python3 setup.py build --ESMFMKFILE=/home/nilik/MYPROGRAMS/LDT/ESMF/lib/libO/Linux.gfortran.64.mpiuni.default/esmf.mk . sudo python3 setup.py install . 7- Check the installation by:&gt; $ python . import ESMF . Compile LDT . Note: If you compile LDT before for new build or new compile LDT you must run &#39;sudo make realclean&#39; in the ldt/make directory before new running . 1- Download LIS NASA from https://lis.gsfc.nasa.gov/ OR https://github.com/NASA-LIS/LISF/releases/tag/v7.3.1-public . 2- Unpack in TOPLEVELDIR . 3- Open terminal in &quot;/home/nilik/MYPROGRAMS/LDT/TOPLEVELDIR/ldt&quot; then: . export LDT_SRC=/home/nilik/MYPROGRAMS/LDT/TOPLEVELDIR/ldt . export LDT_ARCH=linux_gfortran . export LDT_FC=gfortran . export LDT_CC=gcc . export LDT_MODESMF=/home/nilik/MYPROGRAMS/LDT/ESMF/mod/modO/Linux.gfortran.64.mpiuni.default . export LDT_LIBESMF=/home/nilik/MYPROGRAMS/LDT/ESMF/lib/libO/Linux.gfortran.64.mpiuni.default . export LDT_NETCDF=/usr . export LDT_HDF5=/usr/lib/x86_64-linux-gnu/hdf5/serial . export LDT_HDF4=/home/nilik/MYPROGRAMS/hdf4 . export LDT_HDFEOS=/usr/local/hdfeos . export LDT_ECCODES=/usr/local . export LDT_OPENJPEG=/usr . ./configure . Notes: . Before run compile add &quot;-lrt&quot; to the LDFLAGS line of make/configure.ldt and then compile. . AND . For prevent errors when compile such as: &quot;Error: Type mismatch between actual argument at (1) and actual argument at (2)...&quot; etc. . It is neccessary to add . -finit-local-zero -fno-strict-overflow -fallow-argument-mismatch -fallow-invalid-boz . in &quot;FFLAGS&quot; of configure file. . AND . add “-ltirpc” to LDFLAGS for activate XDR in LDT compile. . Example: . FFLAGS = -c -pass-exit-codes -ffree-line-length-0 -O2 -fconvert=big-endian -DGFORTRAN -I$(MOD_ESMF) -I$(INC_ECCODES) -I$(INC_NETCDF) -I$(INC_HDFEOS) -I$(INC_HDF4) -I$(INC_HDF5) -finit-local-zero -fno-strict-overflow -fallow-argument-mismatch -fallow-invalid-boz LDFLAGS = -L$(LIB_ESMF) -lesmf -lstdc++ -lrt -lz -L$(LIB_ECCODES) -leccodes_f90 -leccodes -L$(LIB_JPEG2000) -lopenjp2 -L$(LIB_NETCDF) -lnetcdff -lnetcdf -L$(LIB_HDF5) -lhdf5_fortran -lhdf5_hl -lhdf5 -ldl -ltirpc . 4- . ./compile . OR . ./compile &gt;make.log 2&gt;&amp;1 (for create a log file) . Note:For error of &quot;/usr/bin/env: ‘python’: No such file or directory&quot; when run compile, install python3 with: . sudo apt-get install python-is-python3 . NOTE: After Compiled Finished &quot;LDT&quot; file created in ldt directory. . Using LDT . For use LDT executable file that created in the previous steps, It is best to test LDT with LIS tutorials. . So, for do that the &quot;testcase1_ldt_parms_2020&quot; is best for start. Base on &quot;LDT_Parameters_Testcase_Step1.pdf&quot; of LIS Testcases I did below steps. . 1- Build a new folder as name as &quot;RUNNING&quot; in &quot;/home/nilik/Temp&quot; . 2- Copy contents of &quot;testcase1_ldt_parms_2020&quot; directory to RUNNING. . 3- Copy LDT executable file created to RUNNING. . 4- Open terminal in this folder . 5- type: &quot;:~/RUNNING$ ulimit -s unlimited&quot; . 6- type: &quot;$ ./LDT ldt.config.noah36_params&quot; . 7- ERROR: . &quot;./LDT: error while loading shared libraries: libesmf.so: cannot open shared object file: No such file or directory&quot; . For solve error you must define corrct path for &quot;libesmf.so&quot;, so: . &quot;:~/RUNNING$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/nilik/MYPROGRAMS/LDT/ESMF/lib/libO/Linux.gfortran.64.mpiuni.default&quot; . 8- Again run: . &quot;$ ./LDT ldt.config.noah36_params&quot; . 9- ERROR: . ./LDT: error while loading shared libraries: libeccodes_f90.so: cannot open shared object file: No such file or directory . For solve error you must define correct path for &quot;libeccodes_f90.so&quot;, so: . :~/RUNNING$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/hm/ecCodes/lib . In sum: . :~/RUNNING$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/hm/esmf/lib/libO/Linux.gfortran.64.mpiuni.default :~/RUNNING$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/hm/ecCodes/lib . 10- Again run: . $ ./LDT ldt.config.noah36_params . 11- After a little time the program stopped without any error message while two new files include ldtlog.0000 and MaskParamFill.log created. It seems that there is a problem. So, with comparison of MaskParamFill.log created and target_MaskParamFill.log in &quot;target_log&quot; directory it can be know the problem. . 12- In MaskParamFill.log file created there are only 16 lines but in &quot;target_MaskParamFill.log&quot; file there are more. . 13- In line 17 of target_MaskParamFill.log file: . Checking/filling mask values for: MXSNALBEDO . It seems that the problem may related to Max Snow Albedo. In addition based on answer the quesion in &quot;https://modelingguru.nasa.gov/message/10861#10861&quot;, this problem can be related to HDF4. In previous steps I didn&#39;t use HDF4 for LDT compilation. &quot;HDF4 is required to read the max snow albedo dataset for the first step of the public testcases.&quot; . I reinstalled HDF4 again correctly and run LDT correctly :) . AFTER INSTALL CORRECTLY LIBRARIES AGAIN in UBUNTU-BUDGIE, I CAN RUN FIRST TESTCASE WITH NAME of testcase1_ldt_parms_2020. . NOW I SHOULD USE LDT FOR MY STUDY AREA AND MERRA-2 DATASET . More about LDT . Native Datasets . The Native Model Parameters for LDT. The &quot;Native&quot; model parameters and data sets are obtained from different data providers, including data centers (e.g., NASA&#39;s GES DISC), government and university partners (e.g., NOAA websites). To help users become more familiar with the use of these datasets, several examples are provided in our latest test case suite. These use cases should be helpful in learning how to run LDT and process the &quot;native&quot; parameters onto a target grid and region of interest. . Some examples of &quot;native&quot; parameters and data include: . • MODIS-IGBP landcover • STATSGO+FAO soil texture • SRTM topographic maps (e.g., elevation, slope) • and many other model inputs (e.g., different Noah land model versions) . (src: https://lis.gsfc.nasa.gov/data/ldt/native) . Land surface Data Toolkit . The Land surface Data Toolkit (LDT) is the front-end processor for the Land Information System (LIS), versions 7 and greater. It provides an environment for processing land model data and parameters, as well as restart files and data assimilation based inputs (e.g., for bias correction methods), and other data inputs for LIS and LVT (Arsenault et al., 2018). . LDT also offers a variety of inputs and user options to process datasets and is designed with not only LIS in mind, but for other independent model and modeling systems as well. LDT supports the use of common data formats, like NetCDF, which provide detailed data header information. . Some Major Features . • Processes data inputs for land surface, hydrological, and lake models. • Writes output in NetCDF, a common descriptive format. • Supports multiple observational data sources. • Offers a variety of projections and grid transformation options. • Includes numerous options for processing parameters (e.g., agreement between parameters). • Can function as a stand-alone land surface and data assimilation input processor in addition to being a pre-processor for LIS. . New Data Options . LDT is designed to read &quot;native&quot; model parameters and data files available from their original sources, as well as being backward-compatible with the original LIS-generated binary-formatted parameter datasets. Please note that the LIS team is now moving away from these older LIS binary-formatted files. . Philosophy . LDT has been designed and developed to read in what are considered the &quot;native&quot; or raw original data files as how they are provided by a data center, government agency, university group, etc. This philosophy that data and model parameter files should be read in from their &quot;native&quot; grids and file formats and be written to a common descriptive data format, like NetCDF, is supported throughout all of the LIS framework software. . Capabilities . LDT&#39;s Functional Goals involve developing certain key features, which include: • Process surface model (e.g., land surface models) parameters onto a common grid domain. • Generate model initial conditions (e.g., model ensemble initialization). • Generate CDF statistics that can be used in LIS during data assimilation updates. • Implement and apply quality control measures to parameters, independent validation datasets, etc. • Process and temporally downscale meteorological forcing datasets. • Support for processing observations for data assimilation procedures in LIS. • Machine learning layer being developed and expanded for more applications. . (src: https://lis.gsfc.nasa.gov/software/ldt) . What land surface models (LSMs) are currently supported within the LIS framework? . The current public versions of LIS support the following LSMs: . • Noah 2.7.1 • Noah 3.2 • Noah 3.3 • Noah 3.6 • Noah 3.9 • Noah Multi-Physics (MP), version 3.6 • Catchment LSM (CLSM), Fortuna 2.5 version • VIC 4.1.1 • VIC 4.1.2 • SAC-HTET/SNOW-17 • GeoWRSI 2.0 • CLM 2.0 • Mosaic • HY-SSib . What projections are currently supported in LDT and LIS-7? . Currently, several map projections are supported for both input parameters and the LIS target run grid. These include: &#39;latlon&#39;, &#39;lambert&#39; (for lambert conformal), &#39;polar&#39; (for polar stereographic), &#39;hrap&#39; (for HRAP, a flavor of polar stereographic), &#39;mercator&#39;, and &#39;gaussian&#39; (which is supported in LDT and will be for LIS-7 in future releases). Other projections, like &#39;UTM&#39;, will also be supported in future releases. . What differences are involved with a &quot;readin&quot; landmask vs. wanting to &quot;create&quot; one? . With LDT, LIS users can either &quot;readin&quot; an existing land-water mask or &quot;create&quot; one from the landcover map that is read in (e.g., a landmask derived from MODIS-IGBP, ESA). For most of LIS&#39; history, most users were restricted to using just the &quot;UMD&quot; landmask, which came from the AVHRR satellite-based UMD classification map, circa 1992-1993. Today, LIS users have several different landcover datasets and classifications they can chose from, and from which they can create landmasks. Also, any other landmask can be read in, like the MODIS-based MOD44w landmask product, which is now used in the MODIS Collection 6 products.   Because reading in or creating a landmask map may not share matching land points with other parameter maps read in (e.g., soil texture, elevation), &quot;fill&quot; options are provided in LDT to help ensure agreement  between the mask and the parameter fields. These &quot;fill&quot; routines are based on earlier code used to impose the &quot;UMD&quot; landmask on all of the original LIS-team produced binary files (aka, the &quot;LIS-data&quot; files). Also, the &quot;fill&quot; step occurs on the LIS output run domain after the spatial transform step is performed. . What are the &#39;fill&#39; options (e.g., fill radius)? . The &#39;fill&#39; options associated with each parameter entry in the LDT config file provide the user the ability to make sure the read-in or derived landmask and the read-in parameter files agree (i.e., same number of land points). Continuous type parameter files can utilize either &#39;neighbor&#39; or &#39;average&#39; options in using neighboring pixels to &#39;fill&#39; in a missing parameter value when there is an actual valid land point (i.e., landmask = 1). Discrete data types can only use &#39;neighbor&#39; option at this time (e.g., landcover, soil texture, etc.). Additional options include the &#39;fill radius&#39; which the user can enter the number of row/column pixel area to search for neighboring valid parameter values. If no neighboring values are found (e.g., could be an island), the user can specify a &#39;generic fill&#39; value for that given parameter (e.g., input the land class of 6 for the landcover parameter). . What is a &#39;spatial tranform&#39; and when is it applied? . A &#39;spatial transform&#39; entry in the run-time LDT configure (e.g., ldt.config) file allows the user to specify how a parameter file can be &#39;transformed&#39; or translated from its &#39;native&#39; or original projection and resolution to the designated LIS target grid projection and resolution.   For example, you want to upscale your 0.01°, lat-lon grid landcover file to a 0.25°, lat-lon grid map. You can select &#39;tile&#39; as the entry for the &#39;Landcover spatial transform:&#39; option, and LDT will aggregate the landcover pixels to tile layers (i.e., fraction of each landcover type) within each 0.25° output gridcell. If you were to select &#39;mode&#39; instead here, a value of &quot;1&quot; would be assigned to the most dominant vegetation layer, representing 100% coverage. Since landcover is a discrete data type, you would not want to select interpolation methods, like &#39;bilinear&#39;, or upscaling option, &#39;average&#39; (or you would end up with a landcover type of 4.331!).   For a downscaling example, you could have a 1.0°, lat-lon grid albedo map and you want to run on a 0.25°, lat-lon output grid. You could select &#39;bilinear&#39; for interpolating the coarser albedo parameter map to the finer scale 0.25° map. . What is the &#39;LIS&#39; data format type? . The other type of data read in by LDT includes the LIS team processed files referred to commonly as &#39;LIS&#39; data. These original files are 4-byte real, big-endian, direct access, binary format, with the extension: *.1gd4r . One downside to using these datasets is that many of them have the &quot;UMD&quot; landmask imposed on all the parameter files, forcing users to use this older mask in their model runs. LDT, LIS and LVT are moving away from this data formatted file use. . What are the &#39;Native&#39; data files? . When LDT was being developed, the LIS team decided to provide two pathways for the LIS user community for how data parameters can be read in to LDT and LIS-7. The first being that any original or &#39;native&#39; model parameters and data provided by a government agency, university, organization, etc. should be read in &quot;as-is&quot; and not have any preprocessing done to the files. This &quot;philosophy&quot; is intended to better preserve the original data information and pixel integrity but also allow the user to select how the parameters get processed for their modeling needs. . How does topographic downscaling of the bottom temperature field work? . To capture some of the impact of terrain relief on bottom soil temperature field variability, you can turn the &#39;Bottom temperature topographic downscaling:&#39; entry in the ldt.config file to &quot;lapse-rate&quot;. You will also need to also turn on and read in an elevation map, preferably of higher resolution than your bottom temperature map. Then using the elevation file and the environmental lapse rate (~6.5 K/km), the bottom temperature file values are adjusted by the elevation values, representing what the soil temperature might reflect at a higher or lower elevation. This feature helps then give more detail in mountainous areas and capture local minimum temperatures in the soil temperatures than if not accounted for. . What type of data assimilation inputs does LDT process? . Currently, LDT can estimate the statistics required to do a simple bias-correction or scaling approach between similar observational data and model state estimates to reduce bias between the two during assimilation update step. LDT generates the mean, standard deviation and cumulative (probability) density function (CDF) values, which LIS-7 ingests to perform the final CDF &quot;matching&quot; between the observations and the model estimates. . What LIS DA observation dataset types are supported in LDT and LIS? . The current public LIS versions support the following data assimilation (DA) observation dataset types: . • Soil Moisture Active Passive (SMAP) soil moisture products • NASA/Vrije U. Land Parameter Retrieval Model (LPRM) AMSR-e Soil Moisture data • NASA/NSIDC AMSR-e Soil Moisture data • Essential Climate Variable (ECV) Satellite-based Soil Moisture analysis • TU Wien ASCAT Soil Moisture (retrospective) • NESDIS/OSPO Soil Moisture Operational Products System (SMOPS) Soil Moisture product • WindSat satellite-based Soil Moisture Retrievals • GRACE Terrestrial Water Storage (TWS) Estimates • Synthetic Model-derived Soil Moisture output . What is an ensemble restart file? . An ensemble restart file includes not only one single realization&#39;s set of model state variables required to initialize a LIS model run but several realizations, or ensemble members, to restart a multi-member or open-loop simulation. . What is meant by upscaling or downscaling a restart file? . Upscaling: refers to expanding a single-member (or realization) LIS-generated restart file to an ensemble of members (or model realizations) that can be used for ensemble model runs. This feature could be helpful to someone interested in performing a simulation of an ensemble of model realizations or data assimilation, but have only model member run for one single realization. . Downscaling: refers to averaging or collapsing a multi-member (or -realization) LIS-generated restart file to a single member (or model realization), making it essentially a climatological realization of the member (ensemble mean simulation). .",
            "url": "nilick.github.io/hmbnn-blog/ldt/merra-2/downscale/lis/modeling/2021/06/08/LDT.html",
            "relUrl": "/ldt/merra-2/downscale/lis/modeling/2021/06/08/LDT.html",
            "date": " • Jun 8, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "My notes in Jupyterlab working",
            "content": "Jupyterlab spellchecker . install: . jupyter labextension install @ijmbarr/jupyterlab_spellchecker . Uninstall full program . sudo apt-get purge package-name . sudo apt-get autoremove . Install fonts . sudo apt install git . git clone https://github.com/fzerorubigd/persian-fonts-linux.git . cd persian-fonts-linux . ./farsifonts.sh . Show disk space . df -h . Clean Ubuntu . sudo du -sh /var/cache/apt/archives . sudo apt-get clean . df -h . Install Miniconda . Best resources for install Anaconda or Miniconda with spatial packages* . https://medium.com/@chrieke/howto-install-python-for-geospatial-applications-1dbc82433c05 . Download miniconda python 3.7 (some packages do not work with python 3.8) https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.3-Linux-x86_64.sh . To install Miniconda on Ubuntu 20.04 from command line, it only takes 3 steps excluding creating and activating a conda environment. . 1-Download the latest shell script wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.3-Linux-x86_64.sh . 2-Make the miniconda installation script executable: chmod +x Miniconda3-py37_4.8.3-Linux-x86_64.sh . 3-Run miniconda installation script: ./Miniconda3-py37_4.8.3-Linux-x86_64.sh . Create and activate the conda environment . To create a conda environment, run conda create -n newenv . You can also create the environment from a file like environment.yml, you can use use the conda env create -f command: . conda env create -f environment.yml . The environment name will be the directory name. . Install Packages . Install miniconda | Anaconda Prompt (miniconda3)[Open miniconda] | conda update -n base -c defaults conda [Update Conda installed] | conda install -c anaconda anaconda-navigator [Install Navigator] | Open Anaconda Navigator | Open terminal | conda install -c conda-forge gdal [Install GDAL] | conda install -c anaconda numpy | conda install -c anaconda pandas | conda install -c conda-forge geopandas | conda install -c anaconda xarray | conda install -c conda-forge matplotlib | conda install -c conda-forge cartopy | conda install -c conda-forge descartes [for countries plot in cartopy] | conda install -c conda-forge shapely | conda install -c conda-forge fiona | conda install -c conda-forge pyproj | conda install -c conda-forge bqplot | conda install -c conda-forge ipyleaflet | conda install -c conda-forge nodejsnpm install npm@latest -g [https://nodejs.org/en/] . | conda list | . Install Jupyterlab . -conda install -c conda-forge jupyterlab . -In terminal type: jupyter lab [Open jupyterlab in default browser] . -Ctrl+C or Ctrl+Conda [Close] . -jupyter notebook --generate-config [Change Workdirectory] . open &quot;jupyter_notebook_config.py&quot; file Find &quot; #c.NotebookApp.notebook_dir=&#39; &#39; &quot; Change to &quot; c.NotebookApp.notebook_dir = &#39;~/Anaconda_Projects&#39; &quot; In terminal type: jupyter lab . Install packages in Jupyterlab . jupyter labextension install @jupyter-widgets/jupyterlab-manager jupyter-leaflet | . (https://ipyleaflet.readthedocs.io/en/latest/api_reference/basemaps.html) . -jupyter labextension install @jupyter-widgets/jupyterlab-manager . -jupyter lab build . -jupyter nbextension enable --py widgetsnbextension --sys-prefix . -pip install sidecar . -jupyter labextension install @jupyter-widgets/jupyterlab-manager . -jupyter labextension install @jupyter-widgets/jupyterlab-sidecar . -conda install -c pyviz holoviews bokeh . jupyter labextension install @pyviz/jupyterlab_pyviz . Install Rasterio . rasterio package must be install with gdal, from &quot; https://www.lfd.uci.edu/~gohlke/pythonlibs/#gdal &quot; yuou can find last gdal and rasterio wheel files, then run something like this from the downloads folder: . pip install GDAL-3.1.2-cp39-cp39-win_amd64.whl rasterio-1.1.5-cp39-cp39-win_amd64.whl . conda install -c conda-forge rasterio . [with python 3.7 (rasterio works with this version) Rasterio 1.0.x works with Python versions 2.7.x and 3.5.0 through 3.7.x, and GDAL versions 1.11.x through 2.4.x. Rasterio 1.0.x is not compatible with GDAL versions 3.0.0 or greater.] . -with the below command and after failed and failed, installed finally.&lt;/br&gt; conda install -c https://conda.anaconda.org/ioos rasterio . Cartopy or Basemap . Basemap is going away and being replaced with Cartopy in the near future. For this reason, new python learners are recommended to learn Cartopy. So, install cartopy. . NOTE: DO NOT INSTALL cartopy with basemap, they are conflict. . For UPDATE . pip uninstall -y setuptools . | pip install setuptools . | conda update conda . | conda update -all . | conda update -n base -c defaults conda . | . Run plotly in jupyterlab . For use in Jupyter lab, you will have to install the plotly jupyterlab extension: . jupyter labextension install jupyterlab-plotly . OR . jupyter labextension install @jupyterlab/plotly-extension . jupyter labextension list . jupyter lab build . Then reopen anaconda jupyterlab . Persian fornt . pip install python-bidi . -lpympl . Markdown Formatting . The five most important concepts to format your code appropriately when using markdown are: . Italics: Surround your text with &#39;_&#39; or &#39;*&#39; | Bold: Surround your text with &#39;__&#39; or &#39;**&#39; | inline: Surround your text with &#39;`&#39; | blockquote: Place &#39;&gt;&#39; before your text. . | Links: Surround the text you want to link with &#39;[]&#39; and place the link adjacent to the text, surrounded with &#39;()&#39; | Headings . Notice that including a hashtag before the text in a markdown cell makes the text a heading. The number of hashtags you include will determine the priority of the header (&#39;#&#39; is level one, &#39;##&#39; is level two, &#39;###&#39; is level three and &#39;####&#39; is level four). . no-highlight # H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Alternatively, for H1 and H2, an underline-ish style: Alt-H1 ====== Alt-H2 . H1 . H2 . H3 . H4 . H5 . H6 . Alternatively, for H1 and H2, an underline-ish style: . Alt-H1 . Alt-H2 . Emphasis . no-highlight Emphasis, aka italics, with *asterisks* or _underscores_. Strong emphasis, aka bold, with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough uses two tildes. ~~Scratch this.~~ . Emphasis, aka italics, with asterisks or underscores. . Strong emphasis, aka bold, with asterisks or underscores. . Combined emphasis with asterisks and underscores. . Strikethrough uses two tildes. Scratch this. . Lists . There are three types of lists in markdown. . Ordered list: . Step 1 Step 1B | | Step 3 | Unordered list . CESM-POP | CESM-MOM | CESM-CAM | . Task list . [x] Learn Jupyter Notebooks [x] Writing | [x] Modes | [x] Other Considerations | . | [ ] Submit Paper | . . NOTE: . Double click on each to see how they are built! . . $-b pm sqrt{b^2 - 4ac} over 2a$ $x = a_0 + frac{1}{a_1 + frac{1}{a_2 + frac{1}{a_3 + a_4}}}$ $ forall x in X, quad exists y leq epsilon$ . Shortcuts and tricks . Command Mode Shortcuts . There are a couple of useful keyboard shortcuts in Command Mode that you can leverage to make Jupyter Notebook faster to use. Remember that to switch back and forth between Command Mode and Edit Mode with Esc and Enter. . m: Convert cell to Markdown . y: Convert cell to Code . D+D: Delete cell . o: Toggle between hide or show output . Shift+Arrow up/Arrow down: Selects multiple cells. Once you have selected them you can operate on them like a batch (run, copy, paste etc). . Shift+M: Merge selected cells. . Shift+Tab: [press once] Tells you which parameters to pass on a function . Shift+Tab: [press three times] Gives additional information on the method .",
            "url": "nilick.github.io/hmbnn-blog/jupyterlab/notes/2021/06/07/My-Notes-Jupyterlab.html",
            "relUrl": "/jupyterlab/notes/2021/06/07/My-Notes-Jupyterlab.html",
            "date": " • Jun 7, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Downscaling MERRA-2 Dataset",
            "content": "In this post, I want to describe downscaling of MERRA-2 data in step-by-step. . GlobSim: downscaling global reanalysis . Notes from https://globsim.readthedocs.io/en/latest/ . GlobSim is developed to efficiently download, process and downscale currently available global atmospheric reanalysis data products, which include ERA_Interim from ECMWF, MERRA2 from NASA, and JRA55 from JMA. | GLobSim is only supported on Linux. | The desired meteorological information is specified using simple control files, which prescribe coordinates of sites, elevation, time period, variable lists, and scaling parameters. | Related Posts . Ecological Vulnerability Assessment in Southern Zagros (Part 1) .",
            "url": "nilick.github.io/hmbnn-blog/merra-2/downscale/dataset/climate/2021/03/29/LDT.html",
            "relUrl": "/merra-2/downscale/dataset/climate/2021/03/29/LDT.html",
            "date": " • Mar 29, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Geospatial Datasets Addresses",
            "content": "Climate data . Worldclim https://www.worldclim.org/ | Annual average climate data https://en.tutiempo.net/climate/iran.html | Climate charts https://www.climate-charts.com/ | Global Historical Climatology Network Monthly https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4 | Climate Data Online Search https://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND *** | Climate Scenario Data from the Rossby Centre http://www.smhi.se/en/research/research-departments/climate-research-rossby-centre2-552/climate-scenario-data-from-the-rossby-centre-1.34020 | Upperair Air Data http://weather.uwyo.edu/upperair/ | Merra 2 http://www.soda-pro.com/web-services/meteo-data/merra | Copernicus https://cds.climate.copernicus.eu/#!/home | Eumetsat https://eoportal.eumetsat.int/cas/login?service=https%3A%2F%2Feoportal.eumetsat.int%2FuserMgmt%2Fcallback%3Fclient_name%3DCasClient | Global Surface Summary of the Day - GSOD https://www.ncei.noaa.gov/access/search/data-search/global-summary-of-the-day *** | Integrated Surface Dataset (Global) https://www.ncei.noaa.gov/access/search/data-search/global-hourly *** | NOAA Climate Data https://www.ncdc.noaa.gov/isd/data-access | Surface Data Hourly Global https://www7.ncdc.noaa.gov/CDO/cdopoemain.cmd?datasetabbv=DS3505&amp;countryabbv=&amp;georegionabbv=&amp;resolution=40 | NOAA Dataset Search https://www.ncei.noaa.gov/access/search/dataset-search | Index of ftp://ftp.ncdc.noaa.gov/pub/data/noaa/ ftp://ftp.ncdc.noaa.gov/pub/data/noaa | Free access to NCEI&#39;s archive of global coastal, oceanographic, geophysical, climate, and historical weather data https://www.ncei.noaa.gov/access/search/index?datasetId=global-hourly *** | Renewable point data https://www.renewables.ninja/ | ESA climate office https://climate.esa.int/en/projects/ . | https://cfs.climate.esa.int/index.html#/ . | . Downscaling MERRA2 and other Global Datasets . The Land surface Data Toolkit (LDT v7.2) – a data fusion environment for land data assimilation systems https://lis.gsfc.nasa.gov/ &amp; https://github.com/NASA-LIS/LISF &amp; https://lis.gsfc.nasa.gov/tests/ldt | GlobSim: downscaling global reanalysis /// GlobSim (v1.0): deriving meteorological time series for point locations from multiple global reanalyses https://globsim.readthedocs.io/en/latest/? &amp; https://gmd.copernicus.org/articles/12/4661/2019/ | . Satellite data . VIIRS Plus DMSP Change in Lights (VIIRS+DMSP dLIGHT), v1 (1992, 2002, 2013) https://sedac.ciesin.columbia.edu/data/set/sdei-viirs-dmsp-dlight | Earthexplorer https://earthexplorer.usgs.gov/ | Search Earthdata https://search.earthdata.nasa.gov/search | LANCE: NASA Near Real-Time Data and Imagery https://earthdata.nasa.gov/earth-observation-data/near-real-time | Hazards and Disasters https://earthdata.nasa.gov/earth-observation-data/near-real-time/hazards-and-disasters | Active Fire Data https://earthdata.nasa.gov/earth-observation-data/near-real-time/firms/active-fire-data | Fire Information for Resource Management System (FIRMS) https://earthdata.nasa.gov/earth-observation-data/near-real-time/firms | Worldview Earthdata https://worldview.earthdata.nasa.gov/ | Worldview Snapshots https://wvs.earthdata.nasa.gov/ | Glovis https://glovis.usgs.gov/ | Nasa Earth Observations https://neo.sci.gsfc.nasa.gov/ | EarthNow! Landsat Image Viewer https://earthnow.usgs.gov/observer/ , http://observer.farearth.com/observer/ | SAR Data https://web-services.unavco.org/brokered/ssara/gui | 9 Best Free Land Cover/Land Use Data https://gisgeography.com/free-global-land-cover-land-use-data/ | TerraScope - Free resource for sentinel satellites data https://terrascope.be/en | Assessment of a Future Copernicus Earth Observation Service Component to Support Sustainable Forest Monitoring https://www.reddcopernicus.info/ | Copernicus Biomass https://climate.esa.int/en/projects/biomass/ | Global Forest Watch map https://gfw.global/315sv5h &amp; https://www.globalforestwatch.org/map/?map=eyJkYXRhc2V0cyI6W3siZGF0YXNldCI6Im5ldC1jYXJib24tZmx1eCIsIm9wYWNpdHkiOjEsInZpc2liaWxpdHkiOnRydWUsImxheWVycyI6WyJuZXQtY2FyYm9uLWZsdXgtMjAwMS0yMDE5Il19LHsiZGF0YXNldCI6ImNhcmJvbi1yZW1vdmFscyIsIm9wYWNpdHkiOjEsInZpc2liaWxpdHkiOnRydWUsImxheWVycyI6WyJjYXJib24tcmVtb3ZhbHMtMjAwMS0yMDE5Il19LHsiZGF0YXNldCI6ImNhcmJvbi1lbWlzc2lvbnMiLCJvcGFjaXR5IjoxLCJ2aXNpYmlsaXR5Ijp0cnVlLCJsYXllcnMiOlsiY2FyYm9uLWVtaXNzaW9ucy0yMDAxLTIwMTkiXX0seyJkYXRhc2V0IjoicG9saXRpY2FsLWJvdW5kYXJpZXMiLCJsYXllcnMiOlsiZGlzcHV0ZWQtcG9saXRpY2FsLWJvdW5kYXJpZXMiLCJwb2xpdGljYWwtYm91bmRhcmllcyJdLCJvcGFjaXR5IjoxLCJ2aXNpYmlsaXR5Ijp0cnVlfV19&amp;mapMenu=eyJtZW51U2VjdGlvbiI6ImRhdGFzZXRzIiwiZGF0YXNldENhdGVnb3J5IjoiY2xpbWF0ZSJ9 &amp; https://www.nature.com/articles/s41558-020-00976-6 | SPOT satellites (1, 2, 3, 4 and 5 archive) https://regards.cnes.fr/user/swh/modules/60 | . Sentinel Data . Dashboard https://climate.esa.int/en/odp/#/dashboard | openEO develops an open API to connect R, Python, JavaScript and other clients to big Earth observation cloud back-ends in a simple and unified way. https://openeo.org/ | Sentinell Hub https://scihub.copernicus.eu/dhus/#/home | Sentinel5P https://s5phub.copernicus.eu/dhus/ | Vito Dataset http://www.vito-eodata.be/PDF/portal/Application.html#Home | TerraScope - Free resource for sentinel satellites data https://terrascope.be/en | Assessment of a Future Copernicus Earth Observation Service Component to Support Sustainable Forest Monitoring https://www.reddcopernicus.info/ | Copernicus Biomass https://climate.esa.int/en/projects/biomass/ | Sea Surface Temperature https://climate.esa.int/en/projects/sea-surface-temperature/data/ | Soil Moisture https://climate.esa.int/en/projects/soil-moisture/data/ | Water Vapour https://climate.esa.int/en/projects/water-vapour/data/ | Aerosol https://climate.esa.int/en/projects/aerosol/data/ | Fire https://climate.esa.int/en/projects/fire/data/ | Greenhouse Gases (GHGs) https://climate.esa.int/en/projects/ghgs/Data/ | High Resolution Land Cover https://climate.esa.int/en/projects/high-resolution-land-cover/about/ | Land Cover https://climate.esa.int/en/projects/land-cover/ | Lakes https://climate.esa.int/en/projects/lakes/data/ | Land Surface Temperature https://climate.esa.int/en/projects/land-surface-temperature/ | Ocean Colour https://climate.esa.int/en/projects/ocean-colour/data/ | Ozone https://climate.esa.int/en/projects/ozone/data/ | RECCAP-2 supports and accelerates the analysis of regional carbon budgets https://climate.esa.int/en/projects/reccap-2/ | Sea State https://climate.esa.int/en/projects/sea-state/data/ | Sea Surface Salinity https://climate.esa.int/en/projects/sea-surface-salinity/data/ | . Global Forest Watch map https://gfw.global/315sv5h &amp; https://www.globalforestwatch.org/map/?map=eyJkYXRhc2V0cyI6W3siZGF0YXNldCI6Im5ldC1jYXJib24tZmx1eCIsIm9wYWNpdHkiOjEsInZpc2liaWxpdHkiOnRydWUsImxheWVycyI6WyJuZXQtY2FyYm9uLWZsdXgtMjAwMS0yMDE5Il19LHsiZGF0YXNldCI6ImNhcmJvbi1yZW1vdmFscyIsIm9wYWNpdHkiOjEsInZpc2liaWxpdHkiOnRydWUsImxheWVycyI6WyJjYXJib24tcmVtb3ZhbHMtMjAwMS0yMDE5Il19LHsiZGF0YXNldCI6ImNhcmJvbi1lbWlzc2lvbnMiLCJvcGFjaXR5IjoxLCJ2aXNpYmlsaXR5Ijp0cnVlLCJsYXllcnMiOlsiY2FyYm9uLWVtaXNzaW9ucy0yMDAxLTIwMTkiXX0seyJkYXRhc2V0IjoicG9saXRpY2FsLWJvdW5kYXJpZXMiLCJsYXllcnMiOlsiZGlzcHV0ZWQtcG9saXRpY2FsLWJvdW5kYXJpZXMiLCJwb2xpdGljYWwtYm91bmRhcmllcyJdLCJvcGFjaXR5IjoxLCJ2aXNpYmlsaXR5Ijp0cnVlfV19&amp;mapMenu=eyJtZW51U2VjdGlvbiI6ImRhdGFzZXRzIiwiZGF0YXNldENhdGVnb3J5IjoiY2xpbWF0ZSJ9 &amp; https://www.nature.com/articles/s41558-020-00976-6 | . Terrian . The Global Multi-Resolution Topography (GMRT) Synthesis, Bathymetry and Dem https://www.gmrt.org/GMRTMapTool/ | OpenTopography High-Resolution Topography Data and Tools https://portal.opentopography.org/datasetMetadata | SRTM data https://srtm.csi.cgiar.org/srtmdata/ | ACE2 (Altimeter Corrected Elevations) DEM is a global DEM at 3&quot; (approx 90m at the equator), 9&quot;, 30&#39; (approx 1km at the equator) and 5&#39; resolution http://tethys.cse.dmu.ac.uk/ACE2/shared/main | ACE - Altimeter Corrected Elevations http://www.cse.dmu.ac.uk/EAPRS/products_ace_overview.html | ALOS Global Digital Surface Model &quot;ALOS World 3D - 30m https://www.eorc.jaxa.jp/ALOS/en/aw3d30/ | | . Hydrology . Basins, Rivers, Drainages and etc. https://www.hydrosheds.org/downloads | . Human . Anthropogenic Biomes 2000 https://sedac.ciesin.columbia.edu/data/set/anthromes-anthropogenic-biomes-world-v2-2000 | Global 1-km Downscaled Population Base Year and Projection Grids Based on the SSPs, v1.01 (2000 – 2100) https://sedac.ciesin.columbia.edu/data/set/popdynamics-1-km-downscaled-pop-base-year-projection-ssp-2000-2100-rev01 | Global Rural-Urban Mapping Project (GRUMP) https://sedac.ciesin.columbia.edu/data/set/grump-v1-urban-ext-polygons-rev02 | Socioeconomic Data and Applications Center (sedac) https://sedac.ciesin.columbia.edu/ | . Sea . OpenTopography High-Resolution Topography Data and Tools https://portal.opentopography.org/datasetMetadata | The PO.DAAC: An Open Ocean of Remote Sensing and In Situ Data for Science in the Cloud https://podaac.jpl.nasa.gov/ | Ocean Dataset https://podaac-tools.jpl.nasa.gov/hitide/index.html#id=iQebtv&amp;name=&amp;version=0.1&amp;mapState_x=0.038745394&amp;mapState_y=-0.041098191&amp;mapState_z=2&amp;searchState_startDate=&amp;searchState_endDate=&amp;searchState_bbox= | ERDDAP oceanographic data https://coastwatch.pfeg.noaa.gov/erddap/index.html | Find and download data for your coastal management needs https://coast.noaa.gov/digitalcoast/data/home.html | Digital Coast to Get the Data, Tools https://coast.noaa.gov/digitalcoast/ | NOAA Class https://www.avl.class.noaa.gov/saa/products/welcome;jsessionid=454F7F1B8035F6B94D67B39B1A5367C5 | Bathymetry data sets for the world’s oceans https://www.gebco.net/ | | . Github . Global Map data archives https://github.com/globalmaps | Download MERRA 2 Data https://github.com/emilylaiken/merradownload &amp; https://github.com/TUW-GEO/merra | | . International Organisations . Fao Geonetwork http://www.fao.org/geonetwork/srv/en/main.home | UNEP Environmental Data Explorer http://geodata.grid.unep.ch/ | Integrated Population and Environmental Data https://terra.ipums.org/ | Natural Earthdata, Free vector and raster map data at 1:10m, 1:50m, and 1:110m scales http://www.naturalearthdata.com/downloads/ | Geoportal https://www.geoportal.org/?m:activeLayerTileId=osm&amp;f:dataSource=dab | Google Dataset Search https://datasetsearch.research.google.com/ | ASDC Tools and Services https://eosweb.larc.nasa.gov/tools-and-services | . Atmosphere . Atmospheric Science Data Center https://eosweb.larc.nasa.gov/ | Copernicus https://atmosphere.copernicus.eu/catalogue#/ | . NASA Data . Giovanni https://giovanni.gsfc.nasa.gov/giovanni/ | GES DISC https://disc.gsfc.nasa.gov/datasets?project=MERRA-2 | . Soil . Global maps for Soil Hydraulic Properties https://www.futurewater.eu/2021/06/hihydrosoil-v2-0-now-available-on-google-earth-engine/ | .",
            "url": "nilick.github.io/hmbnn-blog/geospatial/address/dataset/database/2021/03/25/address_urls_geodatabase.html",
            "relUrl": "/geospatial/address/dataset/database/2021/03/25/address_urls_geodatabase.html",
            "date": " • Mar 25, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Ecological Vulnerability Assessment in Southern Zagros (Part 1)",
            "content": "Introduction . The aim of this document is to show the procedure of &quot;Evaluating Ecological Vulnerability Assessment based on Remote Sensing Data in Southern Zagros&quot;. The main sources of document for this research are 1) &quot;Assessment of drought conditions over Vietnam using standardized precipitation evapotranspiration index, MERRA-2 re-analysis, and dynamic land cover&quot; by Manh-HungLe et al. (2020).(Reference). 2) &quot;Method for evaluating ecological vulnerability under climate change based on remote sensing: A case study&quot; by Jiang et al. (2018).(Reference). . Study area . . .",
            "url": "nilick.github.io/hmbnn-blog/vulnerability/zagros/2021/03/22/Ecological-Vulnerability-Assessment.html",
            "relUrl": "/vulnerability/zagros/2021/03/22/Ecological-Vulnerability-Assessment.html",
            "date": " • Mar 22, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "nilick.github.io/hmbnn-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "nilick.github.io/hmbnn-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}